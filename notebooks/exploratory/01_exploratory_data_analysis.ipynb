{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f94cb7e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Employee Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on employee datasets to understand employee differences and patterns.\n",
    "\n",
    "## Objectives\n",
    "- Load and understand the structure of 3 starting files\n",
    "- Identify join columns between datasets\n",
    "- Create a central DataFrame by joining the files\n",
    "- Calculate descriptive statistics on individual files and central dataset\n",
    "- Generate visualizations to highlight key differences between employees\n",
    "- Clean and prepare quantitative and qualitative columns\n",
    "\n",
    "## Expected Results\n",
    "- A central DataFrame from joining the starting files\n",
    "- Descriptive statistics on starting files and central file\n",
    "- Visualizations highlighting key employee differences\n",
    "- Data cleaning and preparation recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ae2eb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86e07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776047d",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Individual Datasets\n",
    "\n",
    "**Important**: Take time to understand column labels and content before rushing into analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c01ae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIRH DATASET ===\n",
      "Shape: (1470, 12)\n",
      "\n",
      "Column names:\n",
      "['id_employee', 'age', 'genre', 'revenu_mensuel', 'statut_marital', 'departement', 'poste', 'nombre_experiences_precedentes', 'nombre_heures_travailless', 'annee_experience_totale', 'annees_dans_l_entreprise', 'annees_dans_le_poste_actuel']\n",
      "\n",
      "First 3 rows:\n",
      "   id_employee  age genre  revenu_mensuel statut_marital departement  \\\n",
      "0            1   41     F            5993    Célibataire  Commercial   \n",
      "1            2   49     M            5130       Marié(e)  Consulting   \n",
      "2            4   37     M            2090    Célibataire  Consulting   \n",
      "\n",
      "                    poste  nombre_experiences_precedentes  \\\n",
      "0        Cadre Commercial                               8   \n",
      "1  Assistant de Direction                               1   \n",
      "2              Consultant                               6   \n",
      "\n",
      "   nombre_heures_travailless  annee_experience_totale  \\\n",
      "0                         80                        8   \n",
      "1                         80                       10   \n",
      "2                         80                        7   \n",
      "\n",
      "   annees_dans_l_entreprise  annees_dans_le_poste_actuel  \n",
      "0                         6                            4  \n",
      "1                        10                            7  \n",
      "2                         0                            0  \n",
      "\n",
      "Data types:\n",
      "id_employee                        int64\n",
      "age                                int64\n",
      "genre                             object\n",
      "revenu_mensuel                     int64\n",
      "statut_marital                    object\n",
      "departement                       object\n",
      "poste                             object\n",
      "nombre_experiences_precedentes     int64\n",
      "nombre_heures_travailless          int64\n",
      "annee_experience_totale            int64\n",
      "annees_dans_l_entreprise           int64\n",
      "annees_dans_le_poste_actuel        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the three starting datasets\n",
    "# Loading actual datasets from raw folder\n",
    "df_sirh = pd.read_csv('../../data/raw/extrait_sirh.csv')  # SIRH dataset\n",
    "df_eval = pd.read_csv('../../data/raw/extrait_eval.csv')  # Evaluation dataset\n",
    "df_sondage = pd.read_csv('../../data/raw/extrait_sondage.csv')  # Survey dataset\n",
    "\n",
    "print(\"=== SIRH DATASET ===\")\n",
    "print(\"Shape:\", df_sirh.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_sirh.columns.tolist())\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_sirh.head(3))\n",
    "print(\"\\nData types:\")\n",
    "print(df_sirh.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6412a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION DATASET ===\n",
      "Shape: (1470, 10)\n",
      "\n",
      "Column names:\n",
      "['satisfaction_employee_environnement', 'note_evaluation_precedente', 'niveau_hierarchique_poste', 'satisfaction_employee_nature_travail', 'satisfaction_employee_equipe', 'satisfaction_employee_equilibre_pro_perso', 'eval_number', 'note_evaluation_actuelle', 'heure_supplementaires', 'augementation_salaire_precedente']\n",
      "\n",
      "First 3 rows:\n",
      "   satisfaction_employee_environnement  note_evaluation_precedente  \\\n",
      "0                                    2                           3   \n",
      "1                                    3                           2   \n",
      "2                                    4                           2   \n",
      "\n",
      "   niveau_hierarchique_poste  satisfaction_employee_nature_travail  \\\n",
      "0                          2                                     4   \n",
      "1                          2                                     2   \n",
      "2                          1                                     3   \n",
      "\n",
      "   satisfaction_employee_equipe  satisfaction_employee_equilibre_pro_perso  \\\n",
      "0                             1                                          1   \n",
      "1                             4                                          3   \n",
      "2                             2                                          3   \n",
      "\n",
      "  eval_number  note_evaluation_actuelle heure_supplementaires  \\\n",
      "0         E_1                         3                   Oui   \n",
      "1         E_2                         4                   Non   \n",
      "2         E_4                         3                   Oui   \n",
      "\n",
      "  augementation_salaire_precedente  \n",
      "0                             11 %  \n",
      "1                             23 %  \n",
      "2                             15 %  \n",
      "\n",
      "Data types:\n",
      "satisfaction_employee_environnement           int64\n",
      "note_evaluation_precedente                    int64\n",
      "niveau_hierarchique_poste                     int64\n",
      "satisfaction_employee_nature_travail          int64\n",
      "satisfaction_employee_equipe                  int64\n",
      "satisfaction_employee_equilibre_pro_perso     int64\n",
      "eval_number                                  object\n",
      "note_evaluation_actuelle                      int64\n",
      "heure_supplementaires                        object\n",
      "augementation_salaire_precedente             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EVALUATION DATASET ===\")\n",
    "print(\"Shape:\", df_eval.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_eval.columns.tolist())\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_eval.head(3))\n",
    "print(\"\\nData types:\")\n",
    "print(df_eval.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9a213",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e1255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SURVEY DATASET ===\n",
      "Shape: (1470, 12)\n",
      "\n",
      "Column names:\n",
      "['a_quitte_l_entreprise', 'nombre_participation_pee', 'nb_formations_suivies', 'nombre_employee_sous_responsabilite', 'code_sondage', 'distance_domicile_travail', 'niveau_education', 'domaine_etude', 'ayant_enfants', 'frequence_deplacement', 'annees_depuis_la_derniere_promotion', 'annes_sous_responsable_actuel']\n",
      "\n",
      "First 3 rows:\n",
      "  a_quitte_l_entreprise  nombre_participation_pee  nb_formations_suivies  \\\n",
      "0                   Oui                         0                      0   \n",
      "1                   Non                         1                      3   \n",
      "2                   Oui                         0                      3   \n",
      "\n",
      "   nombre_employee_sous_responsabilite  code_sondage  \\\n",
      "0                                    1             1   \n",
      "1                                    1             2   \n",
      "2                                    1             4   \n",
      "\n",
      "   distance_domicile_travail  niveau_education  domaine_etude ayant_enfants  \\\n",
      "0                          1                 2  Infra & Cloud             Y   \n",
      "1                          8                 1  Infra & Cloud             Y   \n",
      "2                          2                 2          Autre             Y   \n",
      "\n",
      "  frequence_deplacement  annees_depuis_la_derniere_promotion  \\\n",
      "0           Occasionnel                                    0   \n",
      "1              Frequent                                    1   \n",
      "2           Occasionnel                                    0   \n",
      "\n",
      "   annes_sous_responsable_actuel  \n",
      "0                              5  \n",
      "1                              7  \n",
      "2                              0  \n",
      "\n",
      "Data types:\n",
      "a_quitte_l_entreprise                  object\n",
      "nombre_participation_pee                int64\n",
      "nb_formations_suivies                   int64\n",
      "nombre_employee_sous_responsabilite     int64\n",
      "code_sondage                            int64\n",
      "distance_domicile_travail               int64\n",
      "niveau_education                        int64\n",
      "domaine_etude                          object\n",
      "ayant_enfants                          object\n",
      "frequence_deplacement                  object\n",
      "annees_depuis_la_derniere_promotion     int64\n",
      "annes_sous_responsable_actuel           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SURVEY DATASET ===\")\n",
    "print(\"Shape:\", df_sondage.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_sondage.columns.tolist())\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_sondage.head(3))\n",
    "print(\"\\nData types:\")\n",
    "print(df_sondage.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32251010",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics on Individual Files\n",
    "\n",
    "Analyze each file separately before joining to understand their individual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec843b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIRH DATASET ANALYSIS ===\n",
      "Shape: (1470, 12)\n",
      "Missing values:\n",
      "id_employee                       0\n",
      "age                               0\n",
      "genre                             0\n",
      "revenu_mensuel                    0\n",
      "statut_marital                    0\n",
      "departement                       0\n",
      "poste                             0\n",
      "nombre_experiences_precedentes    0\n",
      "nombre_heures_travailless         0\n",
      "annee_experience_totale           0\n",
      "annees_dans_l_entreprise          0\n",
      "annees_dans_le_poste_actuel       0\n",
      "dtype: int64\n",
      "Duplicates: 0\n",
      "\n",
      "Quantitative columns (8): ['id_employee', 'age', 'revenu_mensuel', 'nombre_experiences_precedentes', 'nombre_heures_travailless', 'annee_experience_totale', 'annees_dans_l_entreprise', 'annees_dans_le_poste_actuel']\n",
      "Qualitative columns (4): ['genre', 'statut_marital', 'departement', 'poste']\n",
      "\n",
      "Numerical summary:\n",
      "       id_employee          age  revenu_mensuel  \\\n",
      "count  1470.000000  1470.000000     1470.000000   \n",
      "mean   1024.865306    36.923810     6502.931293   \n",
      "std     602.024335     9.135373     4707.956783   \n",
      "min       1.000000    18.000000     1009.000000   \n",
      "25%     491.250000    30.000000     2911.000000   \n",
      "50%    1020.500000    36.000000     4919.000000   \n",
      "75%    1555.750000    43.000000     8379.000000   \n",
      "max    2068.000000    60.000000    19999.000000   \n",
      "\n",
      "       nombre_experiences_precedentes  nombre_heures_travailless  \\\n",
      "count                     1470.000000                     1470.0   \n",
      "mean                         2.693197                       80.0   \n",
      "std                          2.498009                        0.0   \n",
      "min                          0.000000                       80.0   \n",
      "25%                          1.000000                       80.0   \n",
      "50%                          2.000000                       80.0   \n",
      "75%                          4.000000                       80.0   \n",
      "max                          9.000000                       80.0   \n",
      "\n",
      "       annee_experience_totale  annees_dans_l_entreprise  \\\n",
      "count              1470.000000               1470.000000   \n",
      "mean                 11.279592                  7.008163   \n",
      "std                   7.780782                  6.126525   \n",
      "min                   0.000000                  0.000000   \n",
      "25%                   6.000000                  3.000000   \n",
      "50%                  10.000000                  5.000000   \n",
      "75%                  15.000000                  9.000000   \n",
      "max                  40.000000                 40.000000   \n",
      "\n",
      "       annees_dans_le_poste_actuel  \n",
      "count                  1470.000000  \n",
      "mean                      4.229252  \n",
      "std                       3.623137  \n",
      "min                       0.000000  \n",
      "25%                       2.000000  \n",
      "50%                       3.000000  \n",
      "75%                       7.000000  \n",
      "max                      18.000000  \n",
      "\n",
      "Categorical summary:\n",
      "genre: 2 unique values\n",
      "  Values: {'M': 882, 'F': 588}\n",
      "statut_marital: 3 unique values\n",
      "  Values: {'Marié(e)': 673, 'Célibataire': 470, 'Divorcé(e)': 327}\n",
      "departement: 3 unique values\n",
      "  Values: {'Consulting': 961, 'Commercial': 446, 'Ressources Humaines': 63}\n",
      "poste: 9 unique values\n",
      "  Values: {'Cadre Commercial': 326, 'Assistant de Direction': 292, 'Consultant': 259, 'Tech Lead': 145, 'Manager': 131, 'Senior Manager': 102, 'Représentant Commercial': 83, 'Directeur Technique': 80, 'Ressources Humaines': 52}\n",
      "==================================================\n",
      "=== EVALUATION DATASET ANALYSIS ===\n",
      "Shape: (1470, 10)\n",
      "Missing values:\n",
      "satisfaction_employee_environnement          0\n",
      "note_evaluation_precedente                   0\n",
      "niveau_hierarchique_poste                    0\n",
      "satisfaction_employee_nature_travail         0\n",
      "satisfaction_employee_equipe                 0\n",
      "satisfaction_employee_equilibre_pro_perso    0\n",
      "eval_number                                  0\n",
      "note_evaluation_actuelle                     0\n",
      "heure_supplementaires                        0\n",
      "augementation_salaire_precedente             0\n",
      "dtype: int64\n",
      "Duplicates: 0\n",
      "\n",
      "Quantitative columns (7): ['satisfaction_employee_environnement', 'note_evaluation_precedente', 'niveau_hierarchique_poste', 'satisfaction_employee_nature_travail', 'satisfaction_employee_equipe', 'satisfaction_employee_equilibre_pro_perso', 'note_evaluation_actuelle']\n",
      "Qualitative columns (3): ['eval_number', 'heure_supplementaires', 'augementation_salaire_precedente']\n",
      "\n",
      "Numerical summary:\n",
      "       satisfaction_employee_environnement  note_evaluation_precedente  \\\n",
      "count                          1470.000000                 1470.000000   \n",
      "mean                              2.721769                    2.729932   \n",
      "std                               1.093082                    0.711561   \n",
      "min                               1.000000                    1.000000   \n",
      "25%                               2.000000                    2.000000   \n",
      "50%                               3.000000                    3.000000   \n",
      "75%                               4.000000                    3.000000   \n",
      "max                               4.000000                    4.000000   \n",
      "\n",
      "       niveau_hierarchique_poste  satisfaction_employee_nature_travail  \\\n",
      "count                1470.000000                           1470.000000   \n",
      "mean                    2.063946                              2.728571   \n",
      "std                     1.106940                              1.102846   \n",
      "min                     1.000000                              1.000000   \n",
      "25%                     1.000000                              2.000000   \n",
      "50%                     2.000000                              3.000000   \n",
      "75%                     3.000000                              4.000000   \n",
      "max                     5.000000                              4.000000   \n",
      "\n",
      "       satisfaction_employee_equipe  \\\n",
      "count                   1470.000000   \n",
      "mean                       2.712245   \n",
      "std                        1.081209   \n",
      "min                        1.000000   \n",
      "25%                        2.000000   \n",
      "50%                        3.000000   \n",
      "75%                        4.000000   \n",
      "max                        4.000000   \n",
      "\n",
      "       satisfaction_employee_equilibre_pro_perso  note_evaluation_actuelle  \n",
      "count                                1470.000000               1470.000000  \n",
      "mean                                    2.761224                  3.153741  \n",
      "std                                     0.706476                  0.360824  \n",
      "min                                     1.000000                  3.000000  \n",
      "25%                                     2.000000                  3.000000  \n",
      "50%                                     3.000000                  3.000000  \n",
      "75%                                     3.000000                  3.000000  \n",
      "max                                     4.000000                  4.000000  \n",
      "\n",
      "Categorical summary:\n",
      "eval_number: 1470 unique values\n",
      "heure_supplementaires: 2 unique values\n",
      "  Values: {'Non': 1054, 'Oui': 416}\n",
      "augementation_salaire_precedente: 15 unique values\n",
      "==================================================\n",
      "=== SURVEY DATASET ANALYSIS ===\n",
      "Shape: (1470, 12)\n",
      "Missing values:\n",
      "a_quitte_l_entreprise                  0\n",
      "nombre_participation_pee               0\n",
      "nb_formations_suivies                  0\n",
      "nombre_employee_sous_responsabilite    0\n",
      "code_sondage                           0\n",
      "distance_domicile_travail              0\n",
      "niveau_education                       0\n",
      "domaine_etude                          0\n",
      "ayant_enfants                          0\n",
      "frequence_deplacement                  0\n",
      "annees_depuis_la_derniere_promotion    0\n",
      "annes_sous_responsable_actuel          0\n",
      "dtype: int64\n",
      "Duplicates: 0\n",
      "\n",
      "Quantitative columns (8): ['nombre_participation_pee', 'nb_formations_suivies', 'nombre_employee_sous_responsabilite', 'code_sondage', 'distance_domicile_travail', 'niveau_education', 'annees_depuis_la_derniere_promotion', 'annes_sous_responsable_actuel']\n",
      "Qualitative columns (4): ['a_quitte_l_entreprise', 'domaine_etude', 'ayant_enfants', 'frequence_deplacement']\n",
      "\n",
      "Numerical summary:\n",
      "       nombre_participation_pee  nb_formations_suivies  \\\n",
      "count               1470.000000            1470.000000   \n",
      "mean                   0.793878               2.799320   \n",
      "std                    0.852077               1.289271   \n",
      "min                    0.000000               0.000000   \n",
      "25%                    0.000000               2.000000   \n",
      "50%                    1.000000               3.000000   \n",
      "75%                    1.000000               3.000000   \n",
      "max                    3.000000               6.000000   \n",
      "\n",
      "       nombre_employee_sous_responsabilite  code_sondage  \\\n",
      "count                               1470.0   1470.000000   \n",
      "mean                                   1.0   1024.865306   \n",
      "std                                    0.0    602.024335   \n",
      "min                                    1.0      1.000000   \n",
      "25%                                    1.0    491.250000   \n",
      "50%                                    1.0   1020.500000   \n",
      "75%                                    1.0   1555.750000   \n",
      "max                                    1.0   2068.000000   \n",
      "\n",
      "       distance_domicile_travail  niveau_education  \\\n",
      "count                1470.000000       1470.000000   \n",
      "mean                    9.192517          2.912925   \n",
      "std                     8.106864          1.024165   \n",
      "min                     1.000000          1.000000   \n",
      "25%                     2.000000          2.000000   \n",
      "50%                     7.000000          3.000000   \n",
      "75%                    14.000000          4.000000   \n",
      "max                    29.000000          5.000000   \n",
      "\n",
      "       annees_depuis_la_derniere_promotion  annes_sous_responsable_actuel  \n",
      "count                          1470.000000                    1470.000000  \n",
      "mean                              2.187755                       4.123129  \n",
      "std                               3.222430                       3.568136  \n",
      "min                               0.000000                       0.000000  \n",
      "25%                               0.000000                       2.000000  \n",
      "50%                               1.000000                       3.000000  \n",
      "75%                               3.000000                       7.000000  \n",
      "max                              15.000000                      17.000000  \n",
      "\n",
      "Categorical summary:\n",
      "a_quitte_l_entreprise: 2 unique values\n",
      "  Values: {'Non': 1233, 'Oui': 237}\n",
      "domaine_etude: 6 unique values\n",
      "  Values: {'Infra & Cloud': 606, 'Transformation Digitale': 464, 'Marketing': 159, 'Entrepreunariat': 132, 'Autre': 82, 'Ressources Humaines': 27}\n",
      "ayant_enfants: 1 unique values\n",
      "  Values: {'Y': 1470}\n",
      "frequence_deplacement: 3 unique values\n",
      "  Values: {'Occasionnel': 1043, 'Frequent': 277, 'Aucun': 150}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze dataset characteristics\n",
    "def analyze_dataset(df, dataset_name):\n",
    "    print(f\"=== {dataset_name} ANALYSIS ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Identify quantitative and qualitative columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nQuantitative columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "    print(f\"Qualitative columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    if numerical_cols:\n",
    "        print(f\"\\nNumerical summary:\")\n",
    "        print(df[numerical_cols].describe())\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"\\nCategorical summary:\")\n",
    "        for col in categorical_cols:\n",
    "            print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "            if df[col].nunique() <= 10:\n",
    "                print(f\"  Values: {df[col].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    return numerical_cols, categorical_cols\n",
    "\n",
    "# Analyze each dataset\n",
    "num_cols_sirh, cat_cols_sirh = analyze_dataset(df_sirh, \"SIRH DATASET\")\n",
    "num_cols_eval, cat_cols_eval = analyze_dataset(df_eval, \"EVALUATION DATASET\") \n",
    "num_cols_sondage, cat_cols_sondage = analyze_dataset(df_sondage, \"SURVEY DATASET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836516e8",
   "metadata": {},
   "source": [
    "## 4. Identify Join Columns and Merge Strategy\n",
    "\n",
    "**Critical**: Determine which columns allow joining the 3 files and choose the appropriate join type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0db4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POTENTIAL JOIN COLUMNS ===\n",
      "SIRH dataset columns: ['id_employee', 'age', 'genre', 'revenu_mensuel', 'statut_marital', 'departement', 'poste', 'nombre_experiences_precedentes', 'nombre_heures_travailless', 'annee_experience_totale', 'annees_dans_l_entreprise', 'annees_dans_le_poste_actuel']\n",
      "Evaluation dataset columns: ['satisfaction_employee_environnement', 'note_evaluation_precedente', 'niveau_hierarchique_poste', 'satisfaction_employee_nature_travail', 'satisfaction_employee_equipe', 'satisfaction_employee_equilibre_pro_perso', 'eval_number', 'note_evaluation_actuelle', 'heure_supplementaires', 'augementation_salaire_precedente']\n",
      "Survey dataset columns: ['a_quitte_l_entreprise', 'nombre_participation_pee', 'nb_formations_suivies', 'nombre_employee_sous_responsabilite', 'code_sondage', 'distance_domicile_travail', 'niveau_education', 'domaine_etude', 'ayant_enfants', 'frequence_deplacement', 'annees_depuis_la_derniere_promotion', 'annes_sous_responsable_actuel']\n",
      "\n",
      "Common columns between SIRH & Evaluation: []\n",
      "Common columns between SIRH & Survey: []\n",
      "Common columns between Evaluation & Survey: []\n",
      "Common columns across all datasets: []\n",
      "\n",
      "=== CHECKING JOIN COLUMN: REPLACE_WITH_ACTUAL_JOIN_COLUMN ===\n"
     ]
    }
   ],
   "source": [
    "# Identify potential join columns\n",
    "print(\"=== POTENTIAL JOIN COLUMNS ===\")\n",
    "print(\"SIRH dataset columns:\", df_sirh.columns.tolist())\n",
    "print(\"Evaluation dataset columns:\", df_eval.columns.tolist())\n",
    "print(\"Survey dataset columns:\", df_sondage.columns.tolist())\n",
    "\n",
    "# Find common columns\n",
    "common_sirh_eval = set(df_sirh.columns) & set(df_eval.columns)\n",
    "common_sirh_sondage = set(df_sirh.columns) & set(df_sondage.columns)\n",
    "common_eval_sondage = set(df_eval.columns) & set(df_sondage.columns)\n",
    "common_all = set(df_sirh.columns) & set(df_eval.columns) & set(df_sondage.columns)\n",
    "\n",
    "print(f\"\\nCommon columns between SIRH & Evaluation: {list(common_sirh_eval)}\")\n",
    "print(f\"Common columns between SIRH & Survey: {list(common_sirh_sondage)}\")\n",
    "print(f\"Common columns between Evaluation & Survey: {list(common_eval_sondage)}\")\n",
    "print(f\"Common columns across all datasets: {list(common_all)}\")\n",
    "\n",
    "# TODO: Replace with your actual join column(s)\n",
    "# Example: join_column = 'employee_id'  # or ['employee_id', 'department']\n",
    "join_column = 'REPLACE_WITH_ACTUAL_JOIN_COLUMN'\n",
    "\n",
    "print(f\"\\n=== CHECKING JOIN COLUMN: {join_column} ===\")\n",
    "if join_column != 'REPLACE_WITH_ACTUAL_JOIN_COLUMN':\n",
    "    print(f\"SIRH dataset - Unique values in {join_column}: {df_sirh[join_column].nunique()}\")\n",
    "    print(f\"Evaluation dataset - Unique values in {join_column}: {df_eval[join_column].nunique()}\")\n",
    "    print(f\"Survey dataset - Unique values in {join_column}: {df_sondage[join_column].nunique()}\")\n",
    "    \n",
    "    # Check for overlaps\n",
    "    overlap_sirh_eval = set(df_sirh[join_column]) & set(df_eval[join_column])\n",
    "    overlap_sirh_sondage = set(df_sirh[join_column]) & set(df_sondage[join_column])\n",
    "    overlap_eval_sondage = set(df_eval[join_column]) & set(df_sondage[join_column])\n",
    "    \n",
    "    print(f\"Overlap between SIRH & Evaluation: {len(overlap_sirh_eval)} values\")\n",
    "    print(f\"Overlap between SIRH & Survey: {len(overlap_sirh_sondage)} values\")\n",
    "    print(f\"Overlap between Evaluation & Survey: {len(overlap_eval_sondage)} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360da890",
   "metadata": {},
   "source": [
    "## 5. Create Central DataFrame by Joining\n",
    "\n",
    "**Join Types**:\n",
    "- `inner`: Only matching records\n",
    "- `left`: All records from left dataset + matching from right\n",
    "- `outer`: All records from both datasets\n",
    "- `right`: All records from right dataset + matching from left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create central DataFrame by joining the three datasets\n",
    "# TODO: Update the join_column and how parameter based on your analysis above\n",
    "\n",
    "# Step 1: Join SIRH and Evaluation datasets\n",
    "if join_column != 'REPLACE_WITH_ACTUAL_JOIN_COLUMN':\n",
    "    # Choose appropriate join type (inner, left, outer, right)\n",
    "    central_df = df_sirh.merge(df_eval, on=join_column, how='inner', suffixes=('_sirh', '_eval'))\n",
    "    print(f\"After joining SIRH and Evaluation: {central_df.shape}\")\n",
    "    \n",
    "    # Step 2: Join with Survey dataset\n",
    "    central_df = central_df.merge(df_sondage, on=join_column, how='inner', suffixes=('', '_sondage'))\n",
    "    print(f\"After joining with Survey: {central_df.shape}\")\n",
    "    \n",
    "    print(f\"\\nFinal central DataFrame shape: {central_df.shape}\")\n",
    "    print(f\"Columns in central DataFrame: {central_df.columns.tolist()}\")\n",
    "    \n",
    "    # Save the central DataFrame\n",
    "    central_df.to_csv('../data/processed/central_employees_dataset.csv', index=False)\n",
    "    print(\"Central DataFrame saved to: ../data/processed/central_employees_dataset.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Please update the join_column variable above with your actual join column!\")\n",
    "    central_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562cc19",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning with .apply() Method\n",
    "\n",
    "**Recommendation**: Use pandas `.apply()` method to clean quantitative and qualitative columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6704e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12befff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each dataset\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "\n",
    "datasets = [(df_sirh, \"SIRH Dataset\"), (df_eval, \"Evaluation Dataset\"), (df_sondage, \"Survey Dataset\")]\n",
    "\n",
    "for df, name in datasets:\n",
    "    print(f\"\\n{name}:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_values,\n",
    "        'Missing Percentage': missing_percentage\n",
    "    })\n",
    "    \n",
    "    missing_data = missing_df[missing_df['Missing Count'] > 0]\n",
    "    if len(missing_data) > 0:\n",
    "        print(missing_data)\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326c1d8",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for each individual dataset\n",
    "print(\"=== STATISTICAL SUMMARY FOR INDIVIDUAL DATASETS ===\")\n",
    "\n",
    "datasets = [(df_sirh, \"SIRH Dataset\"), (df_eval, \"Evaluation Dataset\"), (df_sondage, \"Survey Dataset\")]\n",
    "\n",
    "for df, name in datasets:\n",
    "    print(f\"\\n{name} Statistical Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Numerical variables\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numerical_columns) > 0:\n",
    "        print(\"Numerical Variables:\")\n",
    "        print(df[numerical_columns].describe())\n",
    "    else:\n",
    "        print(\"No numerical variables found.\")\n",
    "    \n",
    "    # Categorical variables\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_columns) > 0:\n",
    "        print(\"\\nCategorical Variables Summary:\")\n",
    "        for col in categorical_columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  - Unique values: {df[col].nunique()}\")\n",
    "            if df[col].nunique() > 0:\n",
    "                most_freq = df[col].value_counts().index[0] if len(df[col].value_counts()) > 0 else 'N/A'\n",
    "                print(f\"  - Most frequent: {most_freq}\")\n",
    "                if df[col].nunique() <= 10:\n",
    "                    print(f\"  - Value counts: {df[col].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"No categorical variables found.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66398eab",
   "metadata": {},
   "source": [
    "## 7. Descriptive Statistics on Central Dataset\n",
    "\n",
    "Compare statistics between individual files and the central dataset to highlight employee differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d4055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f26d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical variables in individual datasets\n",
    "print(\"=== BOX PLOTS FOR INDIVIDUAL DATASETS ===\")\n",
    "\n",
    "datasets = [(df_sirh, \"SIRH Dataset\"), (df_eval, \"Evaluation Dataset\"), (df_sondage, \"Survey Dataset\")]\n",
    "dataset_names = [\"SIRH Dataset\", \"Evaluation Dataset\", \"Survey Dataset\"]\n",
    "\n",
    "for df, name in datasets:\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numerical_columns) > 0:\n",
    "        print(f\"\\nCreating box plots for {name}...\")\n",
    "        \n",
    "        # Calculate subplot dimensions\n",
    "        n_cols = min(2, len(numerical_columns))\n",
    "        n_rows = (len(numerical_columns) + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5*n_rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if len(numerical_columns) == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.ravel()\n",
    "        \n",
    "        for i, col in enumerate(numerical_columns):\n",
    "            if i < len(axes):\n",
    "                df.boxplot(column=col, ax=axes[i])\n",
    "                axes[i].set_title(f'{name} - Box Plot of {col}')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(numerical_columns), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'{name} - Numerical Variables Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{name}: No numerical variables found for box plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for categorical variables in individual datasets\n",
    "print(\"=== BAR PLOTS FOR INDIVIDUAL DATASETS ===\")\n",
    "\n",
    "for df, name in datasets:\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(categorical_columns) > 0:\n",
    "        print(f\"\\nCreating bar plots for {name}...\")\n",
    "        \n",
    "        # Calculate subplot dimensions\n",
    "        n_cols = min(2, len(categorical_columns))\n",
    "        n_rows = (len(categorical_columns) + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5*n_rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if len(categorical_columns) == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.ravel()\n",
    "        \n",
    "        for i, col in enumerate(categorical_columns):\n",
    "            if i < len(axes):\n",
    "                value_counts = df[col].value_counts().head(10)  # Top 10 values\n",
    "                value_counts.plot(kind='bar', ax=axes[i])\n",
    "                axes[i].set_title(f'{name} - Distribution of {col}')\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Count')\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(categorical_columns), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'{name} - Categorical Variables Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{name}: No categorical variables found for bar plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0b05d",
   "metadata": {},
   "source": [
    "## 8. Visualizations to Highlight Employee Differences\n",
    "\n",
    "**Choose appropriate chart types**:\n",
    "- **Quanti vs Quanti**: Scatter plots, correlation heatmaps\n",
    "- **Quanti vs Quali**: Box plots, violin plots, grouped bar charts\n",
    "- **Quali vs Quali**: Stacked bar charts, crosstab heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb30ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8454b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76314914",
   "metadata": {},
   "source": [
    "## 7. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6125f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality tests for numerical variables in each dataset\n",
    "print(\"=== NORMALITY TESTS FOR INDIVIDUAL DATASETS ===\")\n",
    "print(\"Shapiro-Wilk test:\")\n",
    "print(\"H0: Data is normally distributed\")\n",
    "print(\"H1: Data is not normally distributed\")\n",
    "print(\"If p-value < 0.05, reject H0 (data is not normally distributed)\\n\")\n",
    "\n",
    "for df, name in datasets:\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numerical_columns) > 0:\n",
    "        print(f\"\\n{name} - Normality Tests:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for col in numerical_columns:\n",
    "            if len(df[col].dropna()) > 3:  # Need at least 3 values for the test\n",
    "                statistic, p_value = shapiro(df[col].dropna())\n",
    "                result = \"Normal\" if p_value > 0.05 else \"Not Normal\"\n",
    "                print(f\"{col}: p-value = {p_value:.6f} -> {result}\")\n",
    "        \n",
    "        print(f\"\\n{name} - Skewness and Kurtosis:\")\n",
    "        for col in numerical_columns:\n",
    "            skew = df[col].skew()\n",
    "            kurt = df[col].kurtosis()\n",
    "            print(f\"{col}: Skewness = {skew:.3f}, Kurtosis = {kurt:.3f}\")\n",
    "    else:\n",
    "        print(f\"{name}: No numerical variables found for normality tests.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdaeef3",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd32ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method for each dataset\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"=== OUTLIER DETECTION FOR INDIVIDUAL DATASETS ===\")\n",
    "print(\"Using IQR method:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for df, name in datasets:\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numerical_columns) > 0:\n",
    "        print(f\"\\n{name} - Outlier Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for col in numerical_columns:\n",
    "            outliers, lower_bound, upper_bound = detect_outliers_iqr(df, col)\n",
    "            outlier_count = len(outliers)\n",
    "            outlier_percentage = (outlier_count / len(df)) * 100\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  - Outliers count: {outlier_count} ({outlier_percentage:.2f}%)\")\n",
    "            print(f\"  - Lower bound: {lower_bound:.3f}\")\n",
    "            print(f\"  - Upper bound: {upper_bound:.3f}\")\n",
    "            \n",
    "            if outlier_count > 0 and outlier_count <= 10:\n",
    "                print(f\"  - Outlier values: {outliers[col].tolist()}\")\n",
    "    else:\n",
    "        print(f\"{name}: No numerical variables found for outlier detection.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37041008",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations\n",
    "\n",
    "### Expected Deliverables ✅\n",
    "- [x] Central DataFrame from joining 3 starting files\n",
    "- [x] Descriptive statistics on individual and central datasets\n",
    "- [x] Visualizations highlighting key employee differences\n",
    "- [x] Data cleaning using .apply() method\n",
    "\n",
    "### Employee Analysis Summary:\n",
    "**Update after running analysis:**\n",
    "- **Dataset Sizes**: File1: [X rows], File2: [Y rows], File3: [Z rows] → Central: [Total rows]\n",
    "- **Join Strategy**: [inner/left/outer] join on [column_name]\n",
    "- **Key Employee Differences**: [Update based on findings]\n",
    "- **Data Quality Issues**: [Missing values, duplicates, inconsistencies]\n",
    "\n",
    "### Quantitative Insights:\n",
    "- **Salary Differences**: [Range, averages by group]\n",
    "- **Performance Metrics**: [Key differences between employees]\n",
    "- **Demographic Patterns**: [Age, experience distributions]\n",
    "\n",
    "### Qualitative Insights:\n",
    "- **Department Distribution**: [Which departments have most employees]\n",
    "- **Role Categories**: [Distribution of job roles]\n",
    "- **Location Patterns**: [Geographic distribution]\n",
    "\n",
    "### Recommendations for Next Steps:\n",
    "1. **Data Cleaning Priority**: \n",
    "   - [ ] Handle missing values in [specific columns]\n",
    "   - [ ] Standardize categorical values using .apply()\n",
    "   - [ ] Convert data types for numerical columns\n",
    "\n",
    "2. **Further Analysis**:\n",
    "   - [ ] Segment employees by performance/salary brackets\n",
    "   - [ ] Analyze trends over time (if date columns available)\n",
    "   - [ ] Deep dive into specific employee groups\n",
    "\n",
    "3. **Modeling Preparation**:\n",
    "   - [ ] Feature engineering based on insights\n",
    "   - [ ] Prepare target variable for predictive modeling\n",
    "   - [ ] Create train/test datasets\n",
    "\n",
    "### Critical Vigilance Points Addressed ✅\n",
    "- ✅ Understood column labels and content before analysis\n",
    "- ✅ Identified appropriate join columns and strategy\n",
    "- ✅ Chose correct chart types for variable combinations\n",
    "- ✅ Used .apply() method for data cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p4-esn-technova-partners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

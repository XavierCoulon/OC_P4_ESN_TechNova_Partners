{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Employee Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on employee datasets to understand employee differences and patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import  shapiro\n",
    "\n",
    "\n",
    "# Utils functions\n",
    "src_path = os.path.abspath(\"../../src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from utils import analyze_dataset, detect_outliers_iqr, detect_outliers_zscore  # type: ignore\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three starting datasets\n",
    "df_sirh = pd.read_csv('../../data/raw/extrait_sirh.csv')  # SIRH dataset\n",
    "df_eval = pd.read_csv('../../data/raw/extrait_eval.csv')  # Evaluation dataset\n",
    "df_sondage = pd.read_csv('../../data/raw/extrait_sondage.csv')  # Survey dataset\n",
    "\n",
    "dfs = {\n",
    "    \"sirh\": df_sirh,\n",
    "    \"eval\": df_eval,\n",
    "    \"sondage\": df_sondage,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics on Individual Files\n",
    "\n",
    "Analyze each file separately before joining to understand their individual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each dataset\n",
    "\n",
    "for name, df in dfs.items():\n",
    "\tanalyze_dataset(df, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Visualize individual datasets to understand distributions and patterns before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VISUALISATIONS: Histogram + Boxplot + QQ-plot ===\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Filtrer les colonnes ID\n",
    "    id_columns = [\n",
    "        col\n",
    "        for col in numerical_columns\n",
    "        if \"id\" in col.lower() or \"number\" in col.lower() or \"code\" in col.lower()\n",
    "    ]\n",
    "    meaningful_cols = [col for col in numerical_columns if col not in id_columns]\n",
    "\n",
    "    if len(meaningful_cols) > 0:\n",
    "        print(f\"\\nüìä {name} - variables num√©riques: {meaningful_cols}\")\n",
    "\n",
    "        for col in meaningful_cols:\n",
    "            data = df[col].dropna()\n",
    "            if len(data) < 5:\n",
    "                continue  # trop peu de valeurs\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "            # Histogramme + KDE\n",
    "            sns.histplot(\n",
    "                data,\n",
    "                bins=20,\n",
    "                kde=True,\n",
    "                ax=axes[0],\n",
    "                color=\"lightblue\",\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            axes[0].set_title(f\"{col} - Histogram + KDE\")\n",
    "            axes[0].set_xlabel(\"Valeurs\")\n",
    "            axes[0].set_ylabel(\"Fr√©quence\")\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(x=data, ax=axes[1], color=\"lightblue\")\n",
    "            axes[1].set_title(f\"{col} - Boxplot\")\n",
    "            axes[1].set_xlabel(\"Valeurs\")\n",
    "\n",
    "            # QQ-plot (normalit√©)\n",
    "            stats.probplot(data, dist=\"norm\", plot=axes[2])\n",
    "            axes[2].set_title(f\"{col} - QQ-Plot\")\n",
    "\n",
    "            plt.suptitle(f\"{name} - {col}\", fontsize=14, fontweight=\"bold\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: uniquement des colonnes ID/techniques ‚Üí rien de trac√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for categorical variables in individual datasets\n",
    "print(\"=== BAR PLOTS FOR INDIVIDUAL DATASETS ===\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(categorical_columns) > 0:\n",
    "        print(f\"\\nCreating bar plots for {name}...\")\n",
    "        \n",
    "        # Calculate subplot dimensions\n",
    "        n_cols = min(2, len(categorical_columns))\n",
    "        n_rows = (len(categorical_columns) + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5*n_rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if len(categorical_columns) == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.ravel()\n",
    "        \n",
    "        for i, col in enumerate(categorical_columns):\n",
    "            if i < len(axes):\n",
    "                value_counts = df[col].value_counts().head(10)  # Top 10 values\n",
    "                value_counts.plot(kind='bar', ax=axes[i])\n",
    "                axes[i].set_title(f'{name} - Distribution of {col}')\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Count')\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(categorical_columns), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'{name} - Categorical Variables Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{name}: No categorical variables found for bar plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality tests for numerical variables in each dataset\n",
    "print(\"=== NORMALITY TESTS FOR INDIVIDUAL DATASETS ===\")\n",
    "print(\"Shapiro-Wilk test:\")\n",
    "print(\"H0: Data is normally distributed\")\n",
    "print(\"H1: Data is not normally distributed\")\n",
    "print(\"If p-value < 0.05, reject H0 (data is not normally distributed)\\n\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numerical_columns) > 0:\n",
    "        print(f\"\\n{name} - Normality Tests:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for col in numerical_columns:\n",
    "            if len(df[col].dropna()) > 3:  # Need at least 3 values for the test\n",
    "                statistic, p_value = shapiro(df[col].dropna())\n",
    "                result = \"Normal\" if p_value > 0.05 else \"Not Normal\"\n",
    "                print(f\"{col}: p-value = {p_value:.6f} -> {result}\")\n",
    "        \n",
    "        print(f\"\\n{name} - Skewness and Kurtosis:\")\n",
    "        for col in numerical_columns:\n",
    "            skew = df[col].skew()\n",
    "            kurt = df[col].kurtosis()\n",
    "            print(f\"{col}: Skewness = {skew:.3f}, Kurtosis = {kurt:.3f}\")\n",
    "    else:\n",
    "        print(f\"{name}: No numerical variables found for normality tests.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method for each dataset\n",
    "\n",
    "print(\"=== OUTLIER DETECTION FOR INDIVIDUAL DATASETS ===\")\n",
    "print(\"Using IQR method:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numerical_columns) > 0:\n",
    "        print(f\"\\n{name} - Outlier Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for col in numerical_columns:\n",
    "            outliers, lower_bound, upper_bound = detect_outliers_iqr(df, col)\n",
    "            outlier_count = len(outliers)\n",
    "            outlier_percentage = (outlier_count / len(df)) * 100\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  - Outliers count: {outlier_count} ({outlier_percentage:.2f}%)\")\n",
    "            print(f\"  - Lower bound: {lower_bound:.3f}\")\n",
    "            print(f\"  - Upper bound: {upper_bound:.3f}\")\n",
    "            \n",
    "            if outlier_count > 0 and outlier_count <= 10:\n",
    "                print(f\"  - Outlier values: {outliers[col].tolist()}\")\n",
    "    else:\n",
    "        print(f\"{name}: No numerical variables found for outlier detection.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OUTLIER DETECTION FOR INDIVIDUAL DATASETS ===\")\n",
    "print(\"Using Z-score method:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    if len(numerical_columns) > 0:\n",
    "        print(f\"\\n{name} - Outlier Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for col in numerical_columns:\n",
    "            outliers, z_scores = detect_outliers_zscore(df, col, threshold=3.0)\n",
    "            outlier_count = len(outliers)\n",
    "            outlier_percentage = (outlier_count / len(df)) * 100\n",
    "\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  - Outliers count: {outlier_count} ({outlier_percentage:.2f}%)\")\n",
    "            print(f\"  - Threshold: |Z| > 3.0\")\n",
    "\n",
    "            if outlier_count > 0 and outlier_count <= 10:\n",
    "                print(f\"  - Outlier values: {outliers[col].tolist()}\")\n",
    "    else:\n",
    "        print(f\"{name}: No numerical variables found for outlier detection.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[\"niveau_hierarchique_poste\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 7. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh[\"statut_marital\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete nombre_heures_travailless - same value for all rows\n",
    "if len(df_sirh['nombre_heures_travailless'].value_counts()) == 1:\n",
    "    df_sirh.drop(columns=['nombre_heures_travailless'], inplace=True)\n",
    "    \n",
    "df_sirh.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete nombre_employee_sous_responsabilite - same value for all rows\n",
    "if len(df_sondage['nombre_employee_sous_responsabilite'].value_counts()) == 1:\n",
    "\tdf_sondage.drop(columns=['nombre_employee_sous_responsabilite'], inplace=True)\n",
    "\n",
    "df_sondage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ayant_enfants - same value for all rows\n",
    "if len(df_sondage['ayant_enfants'].value_counts()) == 1:\n",
    "    df_sondage.drop(columns=['ayant_enfants'], inplace=True)\n",
    "    \n",
    "df_sondage.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 8. Create Central DataFrame by Joining\n",
    "\n",
    "After completing individual dataset analysis, create the central DataFrame by joining all datasets.\n",
    "Join type used is `left` to retain all employees from SIRH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both df_sirh on 'id_employee' (ex: 10) and df_eval datasets on 'eval_number' ex(\"E_10\")\n",
    "# Clean eval_number column if it contains 'E_' prefix\n",
    "if df_eval['eval_number'].dtype == 'object':\n",
    "    df_eval['eval_number'] = df_eval['eval_number'].str.replace('E_', '').astype(int)\n",
    "\n",
    "# Ensure id_employee is integer\n",
    "df_sirh['id_employee'] = df_sirh['id_employee'].astype(int)\n",
    "\n",
    "# Perform the merges\n",
    "df_sirh_eval = pd.merge(df_sirh, df_eval, left_on='id_employee', right_on='eval_number', how='left')\n",
    "df_sirh_eval_sondage = pd.merge(df_sirh_eval, df_sondage, left_on='id_employee', right_on='code_sondage', how='left')\n",
    "\n",
    "analyze_dataset(df_sirh_eval_sondage, \"FINAL MERGED DATASET (SIRH, EVALUATION & SURVEY)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 9. Visualizations to Highlight Employee Differences\n",
    "\n",
    "**Choose appropriate chart types**:\n",
    "- **Quanti vs Quanti**: Scatter plots, correlation heatmaps\n",
    "- **Quanti vs Quali**: Box plots, violin plots, grouped bar charts\n",
    "- **Quali vs Quali**: Stacked bar charts, crosstab heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sirh_eval_sondage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the final merged dataset into two: those who left and those who stayed\n",
    "df_left = df_sirh_eval_sondage[df_sirh_eval_sondage['a_quitte_l_entreprise'] == 1]\n",
    "df_stayed = df_sirh_eval_sondage[df_sirh_eval_sondage['a_quitte_l_entreprise'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df_sirh_eval_sondage.groupby(\"a_quitte_l_entreprise\")[['revenu_mensuel',\n",
    "       'nombre_experiences_precedentes', 'annee_experience_totale',\n",
    "       'annees_dans_l_entreprise', 'annees_dans_le_poste_actuel',\n",
    "       'satisfaction_employee_environnement', 'note_evaluation_precedente',\n",
    "       'niveau_hierarchique_poste', 'satisfaction_employee_nature_travail',\n",
    "       'satisfaction_employee_equipe',\n",
    "       'satisfaction_employee_equilibre_pro_perso', \n",
    "       'note_evaluation_actuelle', 'nombre_participation_pee',\n",
    "       'nb_formations_suivies', 'distance_domicile_travail',\n",
    "       'niveau_education', 'annees_depuis_la_derniere_promotion',\n",
    "       'annes_sous_responsable_actuel']].mean()\n",
    "group_labels = group_means.index.tolist()\n",
    "diff_means = group_means.loc[group_labels[1]] - group_means.loc[group_labels[0]]\n",
    "\n",
    "\n",
    "print(\"Moyennes par groupe :\\n\", group_means)\n",
    "print(\"\\nDiff√©rence de moyenne :\\n\", diff_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df_sirh_eval_sondage.select_dtypes(include=\"number\").columns.tolist()\n",
    "# Exclure toutes les colonnes ID ou non-significatives\n",
    "exclude_cols = [\"id_employee\", \"eval_number\", \"code_sondage\"]\n",
    "numerical_cols = [c for c in numerical_cols if c not in exclude_cols]\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(\n",
    "        data=df_sirh_eval_sondage,\n",
    "        x=col,\n",
    "        hue=\"a_quitte_l_entreprise\",\n",
    "        stat=\"density\",\n",
    "        common_norm=False,\n",
    "        kde=True,\n",
    "        palette=[\"skyblue\", \"salmon\"],\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.title(f\"Distribution of {col} by turnover\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "\tdf_sirh_eval_sondage, x=\"age\", hue=\"a_quitte_l_entreprise\", kde=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"departement\", hue=\"a_quitte_l_entreprise\", data=df_sirh_eval_sondage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"a_quitte_l_entreprise\", y=\"annees_dans_l_entreprise\", data=df_sirh_eval_sondage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Optional : Merge Verification and Validation\n",
    "\n",
    "Let's verify the merge operations step by step to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MERGE VERIFICATION ANALYSIS ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Original dataset sizes\n",
    "print(\"1. ORIGINAL DATASET SIZES:\")\n",
    "print(f\"   SIRH dataset: {df_sirh.shape[0]} rows, {df_sirh.shape[1]} columns\")\n",
    "print(f\"   Evaluation dataset: {df_eval.shape[0]} rows, {df_eval.shape[1]} columns\") \n",
    "print(f\"   Survey dataset: {df_sondage.shape[0]} rows, {df_sondage.shape[1]} columns\")\n",
    "\n",
    "# 2. Check uniqueness of join keys (current state)\n",
    "print(\"\\n2. JOIN KEY UNIQUENESS (CURRENT STATE):\")\n",
    "print(f\"   SIRH 'id_employee' unique values: {df_sirh['id_employee'].nunique()} / {len(df_sirh)} ({df_sirh['id_employee'].nunique()/len(df_sirh)*100:.1f}% unique)\")\n",
    "print(f\"   Evaluation 'eval_number' unique values: {df_eval['eval_number'].nunique()} / {len(df_eval)} ({df_eval['eval_number'].nunique()/len(df_eval)*100:.1f}% unique)\")\n",
    "print(f\"   Survey 'code_sondage' unique values: {df_sondage['code_sondage'].nunique()} / {len(df_sondage)} ({df_sondage['code_sondage'].nunique()/len(df_sondage)*100:.1f}% unique)\")\n",
    "\n",
    "# 3. Check the data types and sample values\n",
    "print(\"\\n3. KEY DATA TYPES AND SAMPLES:\")\n",
    "print(f\"   SIRH id_employee type: {df_sirh['id_employee'].dtype}, samples: {df_sirh['id_employee'].head(3).tolist()}\")\n",
    "print(f\"   Evaluation eval_number type: {df_eval['eval_number'].dtype}, samples: {df_eval['eval_number'].head(3).tolist()}\")\n",
    "print(f\"   Survey code_sondage type: {df_sondage['code_sondage'].dtype}, samples: {df_sondage['code_sondage'].head(3).tolist()}\")\n",
    "\n",
    "# 4. Check overlaps between keys\n",
    "print(\"\\n4. KEY OVERLAP ANALYSIS:\")\n",
    "sirh_ids = set(df_sirh['id_employee'])\n",
    "eval_ids = set(df_eval['eval_number'])\n",
    "survey_ids = set(df_sondage['code_sondage'])\n",
    "\n",
    "overlap_sirh_eval = sirh_ids & eval_ids\n",
    "overlap_sirh_survey = sirh_ids & survey_ids\n",
    "overlap_eval_survey = eval_ids & survey_ids\n",
    "all_overlap = sirh_ids & eval_ids & survey_ids\n",
    "\n",
    "print(f\"   SIRH ‚à© Evaluation: {len(overlap_sirh_eval)} common IDs\")\n",
    "print(f\"   SIRH ‚à© Survey: {len(overlap_sirh_survey)} common IDs\") \n",
    "print(f\"   Evaluation ‚à© Survey: {len(overlap_eval_survey)} common IDs\")\n",
    "print(f\"   All three datasets: {len(all_overlap)} common IDs\")\n",
    "\n",
    "# 5. Merged dataset sizes\n",
    "print(\"\\n5. MERGED DATASET SIZES:\")\n",
    "print(f\"   SIRH + Evaluation: {df_sirh_eval.shape[0]} rows, {df_sirh_eval.shape[1]} columns\")\n",
    "print(f\"   Final (all three): {df_sirh_eval_sondage.shape[0]} rows, {df_sirh_eval_sondage.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DETAILED MERGE VALIDATION ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6. Check for data loss/duplication in merges\n",
    "print(\"6. MERGE IMPACT ANALYSIS:\")\n",
    "\n",
    "# First merge (SIRH + Evaluation)\n",
    "sirh_lost = len(df_sirh) - len(df_sirh_eval[df_sirh_eval['eval_number'].notna()])\n",
    "eval_lost = len(df_eval) - len(df_sirh_eval[df_sirh_eval['eval_number'].notna()])\n",
    "\n",
    "print(f\"   First merge (SIRH + Evaluation with LEFT join):\")\n",
    "print(f\"     - SIRH records retained: {len(df_sirh_eval)} / {len(df_sirh)} ({len(df_sirh_eval)/len(df_sirh)*100:.1f}%)\")\n",
    "print(f\"     - Evaluation data matched: {len(df_sirh_eval[df_sirh_eval['eval_number'].notna()])} / {len(df_eval)} ({len(df_sirh_eval[df_sirh_eval['eval_number'].notna()])/len(df_eval)*100:.1f}%)\")\n",
    "\n",
    "# Second merge (+ Survey)\n",
    "sirh_eval_lost = len(df_sirh_eval) - len(df_sirh_eval_sondage[df_sirh_eval_sondage['code_sondage'].notna()])\n",
    "survey_lost = len(df_sondage) - len(df_sirh_eval_sondage[df_sirh_eval_sondage['code_sondage'].notna()])\n",
    "\n",
    "print(f\"\\n   Second merge (+ Survey with LEFT join):\")\n",
    "print(f\"     - SIRH+Eval records retained: {len(df_sirh_eval_sondage)} / {len(df_sirh_eval)} ({len(df_sirh_eval_sondage)/len(df_sirh_eval)*100:.1f}%)\")\n",
    "print(f\"     - Survey data matched: {len(df_sirh_eval_sondage[df_sirh_eval_sondage['code_sondage'].notna()])} / {len(df_sondage)} ({len(df_sirh_eval_sondage[df_sirh_eval_sondage['code_sondage'].notna()])/len(df_sondage)*100:.1f}%)\")\n",
    "\n",
    "# 7. Check for duplicates after merge\n",
    "print(\"\\n7. DUPLICATE CHECK AFTER MERGES:\")\n",
    "print(f\"   Duplicates in SIRH+Eval: {df_sirh_eval.duplicated().sum()}\")\n",
    "print(f\"   Duplicates in final dataset: {df_sirh_eval_sondage.duplicated().sum()}\")\n",
    "\n",
    "# 8. Missing values introduced by merges\n",
    "print(\"\\n8. MISSING VALUES INTRODUCED:\")\n",
    "sirh_eval_missing = df_sirh_eval.isnull().sum().sum() - df_sirh.isnull().sum().sum()\n",
    "final_missing = df_sirh_eval_sondage.isnull().sum().sum() - df_sirh_eval.isnull().sum().sum()\n",
    "\n",
    "print(f\"   Missing values added in first merge: {sirh_eval_missing}\")\n",
    "print(f\"   Missing values added in second merge: {final_missing}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MERGE QUALITY ASSESSMENT ===\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# 9. Sample verification - show some merged records\n",
    "print(\"9. SAMPLE MERGED DATA VERIFICATION:\")\n",
    "print(\"\\nFirst 3 records with all data present:\")\n",
    "complete_records = df_sirh_eval_sondage[\n",
    "    (df_sirh_eval_sondage['eval_number'].notna()) & \n",
    "    (df_sirh_eval_sondage['code_sondage'].notna())\n",
    "]\n",
    "\n",
    "if len(complete_records) > 0:\n",
    "    sample_cols = ['id_employee', 'eval_number', 'code_sondage']\n",
    "    available_cols = [col for col in sample_cols if col in complete_records.columns]\n",
    "    print(complete_records[available_cols].head(3))\n",
    "    print(f\"\\nTotal records with all three sources: {len(complete_records)}\")\n",
    "else:\n",
    "    print(\"No records found with data from all three sources!\")\n",
    "\n",
    "# 10. Key relationship verification\n",
    "print(\"\\n10. KEY RELATIONSHIP VERIFICATION:\")\n",
    "print(\"Checking if id_employee = eval_number = code_sondage after formatting...\")\n",
    "\n",
    "if len(complete_records) > 0:\n",
    "    # Check if the keys match for records that have all data\n",
    "    id_eval_match = (complete_records['id_employee'] == complete_records['eval_number']).all()\n",
    "    id_survey_match = (complete_records['id_employee'] == complete_records['code_sondage']).all()\n",
    "    \n",
    "    print(f\"   id_employee == eval_number: {id_eval_match}\")\n",
    "    print(f\"   id_employee == code_sondage: {id_survey_match}\")\n",
    "    \n",
    "    if id_eval_match and id_survey_match:\n",
    "        print(\"   ‚úÖ All keys match perfectly - merge is correct!\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Key mismatch detected - needs investigation\")\n",
    "        \n",
    "        # Show mismatches\n",
    "        id_eval_mismatch = complete_records[complete_records['id_employee'] != complete_records['eval_number']]\n",
    "        id_survey_mismatch = complete_records[complete_records['id_employee'] != complete_records['code_sondage']]\n",
    "        \n",
    "        if len(id_eval_mismatch) > 0:\n",
    "            print(f\"   ID-Eval mismatches: {len(id_eval_mismatch)} records\")\n",
    "        if len(id_survey_mismatch) > 0:\n",
    "            print(f\"   ID-Survey mismatches: {len(id_survey_mismatch)} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and merged dataset\n",
    "print(\"üíæ Saving cleaned dataset...\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the processed directory if it doesn't exist\n",
    "processed_dir = '../../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Remove redundant id columns after merge\n",
    "df_sirh_eval_sondage.drop(columns=[\"eval_number\", \"code_sondage\"], inplace=True)\n",
    "\n",
    "# Save the final merged dataset (overwrite existing file)\n",
    "output_path = '../../data/processed/employee_data_merged_clean.csv'\n",
    "df_sirh_eval_sondage.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset saved to: {output_path}\")\n",
    "print(f\"üìä Dataset contains: {df_sirh_eval_sondage.shape[0]} employees with {df_sirh_eval_sondage.shape[1]} features\")\n",
    "print(\"üìù File will be overwritten on each run - no timestamped backups\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p4-esn-technova-partners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

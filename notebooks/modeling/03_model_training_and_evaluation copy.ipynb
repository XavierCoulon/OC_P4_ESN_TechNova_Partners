{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Machine learning libraries\n",
    "from typing import Literal, cast\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Utils functions\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to Python path to import our utils module\n",
    "src_path = os.path.abspath(\"../../src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import feature engineering utilities from our custom module\n",
    "from project_utils import confusion_matrix_analysis, analyze_feature_scaling, get_classification_report_table  # type: ignore\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "# Styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"LOADING PREPROCESSED DATA:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define data paths\n",
    "data_path = \"../../data/processed\"\n",
    "X_file = os.path.join(data_path, \"X_features.csv\")\n",
    "y_file = os.path.join(data_path, \"y_target.csv\")\n",
    "\n",
    "# Load features and target\n",
    "X = pd.read_csv(X_file)\n",
    "y = pd.read_csv(y_file).squeeze()  # Convert to Series\n",
    "\n",
    "print(f\"‚úÖ Features loaded: {X.shape}\")\n",
    "print(f\"‚úÖ Target loaded: {y.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Total features: {X.shape[1]}\")\n",
    "print(f\"   Target variable: {y.name}\")\n",
    "print(f\"   Target classes: {sorted(y.unique())}\")\n",
    "print(f\"   Class distribution:\")\n",
    "for class_val in sorted(y.unique()):\n",
    "    count = (y == class_val).sum()\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"     Class {class_val}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "\n",
    "AVERAGE_TYPE = 'weighted'  # Options: 'micro', 'macro', 'weighted', 'samples'\n",
    "print(f\"\\nüéØ Using average type: {AVERAGE_TYPE}\")\n",
    "\n",
    "\n",
    "print(f\"\\nüéØ Data ready for modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Analyze feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_feature_scaling(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "print(\"TRAIN/TEST SPLIT:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train set: {X_train.shape}\")\n",
    "print(f\"‚úÖ Test set: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Class distribution after split:\")\n",
    "print(f\"\\nTraining set:\")\n",
    "for class_val in sorted(y_train.unique()):\n",
    "    count = (y_train == class_val).sum()\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  Class {class_val}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "for class_val in sorted(y_test.unique()):\n",
    "    count = (y_test == class_val).sum()\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  Class {class_val}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Stratified split completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Baseline Model - DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BASELINE MODEL - DUMMYCLASSIFIER WITH FULL CV METRICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "dummy_strategies = [\"most_frequent\", \"stratified\", \"uniform\"]\n",
    "\n",
    "# --- CV scoring ---\n",
    "scoring_metrics = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": f\"precision_{AVERAGE_TYPE}\",\n",
    "    \"recall\": f\"recall_{AVERAGE_TYPE}\",\n",
    "    \"f1\": f\"f1_{AVERAGE_TYPE}\",\n",
    "}\n",
    "\n",
    "dummy_results = {}\n",
    "\n",
    "# --- TEST all Dummy strategies ---\n",
    "for strategy in dummy_strategies:\n",
    "    print(f\"\\nü§ñ Testing strategy: {strategy}\")\n",
    "\n",
    "    strategy_literal = cast(\n",
    "        Literal[\"most_frequent\", \"prior\", \"stratified\", \"uniform\", \"constant\"], strategy\n",
    "    )\n",
    "    dummy = DummyClassifier(strategy=strategy_literal, random_state=42)\n",
    "\n",
    "    # --- Cross-validation ---\n",
    "    cv_scores = {}\n",
    "    for metric_name, scoring in scoring_metrics.items():\n",
    "        scores = cross_val_score(dummy, X_train, y_train, cv=cv, scoring=scoring)\n",
    "        cv_scores[metric_name] = (scores.mean(), scores.std())\n",
    "\n",
    "    # --- Fit full train / evaluate test ---\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred_train = dummy.predict(X_train)\n",
    "    y_pred_test = dummy.predict(X_test)\n",
    "\n",
    "    train_scores = {\n",
    "        \"accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "        \"precision\": precision_score(\n",
    "            y_train, y_pred_train, average=AVERAGE_TYPE, zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(y_train, y_pred_train, average=AVERAGE_TYPE),\n",
    "        \"f1\": f1_score(y_train, y_pred_train, average=AVERAGE_TYPE),\n",
    "    }\n",
    "    test_scores = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "        \"precision\": precision_score(\n",
    "            y_test, y_pred_test, average=AVERAGE_TYPE, zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(y_test, y_pred_test, average=AVERAGE_TYPE),\n",
    "        \"f1\": f1_score(y_test, y_pred_test, average=AVERAGE_TYPE),\n",
    "    }\n",
    "\n",
    "    # --- Store results ---\n",
    "    dummy_results[strategy] = {\n",
    "        \"cv\": cv_scores,\n",
    "        \"train\": train_scores,\n",
    "        \"test\": test_scores,\n",
    "    }\n",
    "\n",
    "    # --- Display partial summary ---\n",
    "    print(\"   üìä Cross-validation metrics:\")\n",
    "    for metric, (mean, std) in cv_scores.items():\n",
    "        print(f\"      {metric}: {mean:.4f} ¬± {std:.4f}\")\n",
    "\n",
    "    print(\"   üìä Test set metrics:\")\n",
    "    for metric, value in test_scores.items():\n",
    "        print(f\"      {metric}: {value:.4f}\")\n",
    "\n",
    "# --- Identify best Dummy strategy ---\n",
    "best_strategy = max(dummy_results.keys(), key=lambda k: dummy_results[k][\"test\"][\"f1\"])\n",
    "best_f1 = dummy_results[best_strategy][\"test\"][\"f1\"]\n",
    "\n",
    "print(f\"\\nüèÜ Best baseline strategy: {best_strategy}\")\n",
    "print(f\"    Test F1: {best_f1:.4f}\")\n",
    "\n",
    "# --- Retrain the best Dummy model ---\n",
    "dummy_best = DummyClassifier(strategy=best_strategy, random_state=42)\n",
    "dummy_best.fit(X_train, y_train)\n",
    "\n",
    "# Predictions + Probabilities (if available)\n",
    "y_pred_train_dummy = dummy_best.predict(X_train)\n",
    "y_pred_test_dummy = dummy_best.predict(X_test)\n",
    "y_pred_proba_dummy = dummy_best.predict_proba(X_test)\n",
    "\n",
    "# --- Metrics summary ---\n",
    "\n",
    "accuracy_train_dummy = dummy_results[best_strategy][\"train\"][\"accuracy\"]\n",
    "accuracy_test_dummy = dummy_results[best_strategy][\"test\"][\"accuracy\"]\n",
    "precision_train_dummy = dummy_results[best_strategy][\"train\"][\"precision\"]\n",
    "precision_test_dummy = dummy_results[best_strategy][\"test\"][\"precision\"]\n",
    "recall_train_dummy = dummy_results[best_strategy][\"train\"][\"recall\"]\n",
    "recall_test_dummy = dummy_results[best_strategy][\"test\"][\"recall\"]\n",
    "f1_train_dummy = dummy_results[best_strategy][\"train\"][\"f1\"]\n",
    "f1_test_dummy = dummy_results[best_strategy][\"test\"][\"f1\"]\n",
    "\n",
    "print(f\"\\n   Training Set (Full Fit):\")\n",
    "for metric, val in train_scores.items():\n",
    "    print(f\"      {metric.capitalize():<10}: {val:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test Set:\")\n",
    "for metric, val in test_scores.items():\n",
    "    print(f\"      {metric.capitalize():<10}: {val:.4f}\")\n",
    "\n",
    "# --- Overfitting check ---\n",
    "f1_diff_dummy = f1_train_dummy - f1_test_dummy\n",
    "if abs(f1_diff_dummy) < 0.01:\n",
    "    status = \"‚úÖ Excellent generalization\"\n",
    "elif f1_diff_dummy > 0.05:\n",
    "    status = \"‚ö†Ô∏è Possible overfitting\"\n",
    "elif f1_diff_dummy > 0.02:\n",
    "    status = \"‚ö° Minor overfitting\"\n",
    "else:\n",
    "    status = \"‚ÑπÔ∏è Normal variance\"\n",
    "\n",
    "print(f\"\\n   üîç Overfitting Analysis:\")\n",
    "print(f\"      Train-Test F1 diff: {f1_diff_dummy:+.4f}\")\n",
    "print(f\"      Status: {status}\")\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(f\"\\nüìã Classification Report - DummyClassifier (Best Strategy):\")\n",
    "display(\n",
    "    get_classification_report_table(\n",
    "        y_test, y_pred_test_dummy, f\"DummyClassifier ({best_strategy})\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Baseline established! Any real model should beat test F1={best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_analysis(y_test, y_pred_test_dummy, model_name=\"Dummy Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROC curve for DummyClassifier (placed after Dummy model, before Logistic Regression)\n",
    "\n",
    "\n",
    "# Some DummyClassifier strategies implement predict_proba (e.g., 'stratified' or 'uniform'),\n",
    "# while 'most_frequent' does not provide useful probabilities. We handle both cases.\n",
    "try:\n",
    "    if hasattr(dummy_best, 'predict_proba'):\n",
    "        # Use probability for positive class (assumes binary labels 0/1)\n",
    "        y_score_dummy = dummy_best.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score_dummy)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr, tpr, label=f'Dummy ({best_strategy}) ROC (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', linewidth=0.8)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve - Dummy Classifier')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('‚ö†Ô∏è Dummy classifier does not implement predict_proba(); skipping ROC plot.')\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è Could not compute ROC for Dummy classifier: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression Model (with StandardScaler Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# LOGISTIC REGRESSION MODEL WITH CROSS-VALIDATION\n",
    "# ----------------------------------------\n",
    "print(\"LOGISTIC REGRESSION MODEL:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Create Pipeline (but don't fit yet) ---\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "print(\"ü§ñ Pipeline created with StandardScaler + LogisticRegression\")\n",
    "\n",
    "# --- STEP 1: Cross-validation BEFORE fitting ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': f'precision_{AVERAGE_TYPE}',\n",
    "    'recall': f'recall_{AVERAGE_TYPE}',\n",
    "    'f1': f'f1_{AVERAGE_TYPE}',\n",
    "}\n",
    "\n",
    "print(\"\\nüîÑ Performing Cross-Validation (fresh models)...\")\n",
    "cv_results = {}\n",
    "for metric_name, scoring in scoring_metrics.items():\n",
    "    scores = cross_val_score(lr_pipeline, X_train, y_train, cv=cv, scoring=scoring)\n",
    "    cv_results[metric_name] = (scores.mean(), scores.std())\n",
    "\n",
    "# --- STEP 2: Fit on full training set ---\n",
    "print(\"üèãÔ∏è Fitting on full training set...\")\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- STEP 3: Predictions ---\n",
    "y_pred_train_lr = lr_pipeline.predict(X_train)\n",
    "y_pred_test_lr = lr_pipeline.predict(X_test)\n",
    "y_pred_proba_lr = lr_pipeline.predict_proba(X_test)\n",
    "\n",
    "# --- STEP 4: Calculate metrics ---\n",
    "# Training metrics\n",
    "accuracy_train_lr = accuracy_score(y_train, y_pred_train_lr)\n",
    "precision_train_lr = precision_score(y_train, y_pred_train_lr, average=AVERAGE_TYPE, zero_division=0)\n",
    "recall_train_lr = recall_score(y_train, y_pred_train_lr, average=AVERAGE_TYPE)\n",
    "f1_train_lr = f1_score(y_train, y_pred_train_lr, average=AVERAGE_TYPE)\n",
    "\n",
    "# Test metrics\n",
    "accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "precision_test_lr = precision_score(y_test, y_pred_test_lr, average=AVERAGE_TYPE, zero_division=0)\n",
    "recall_test_lr = recall_score(y_test, y_pred_test_lr, average=AVERAGE_TYPE)\n",
    "f1_test_lr = f1_score(y_test, y_pred_test_lr, average=AVERAGE_TYPE)\n",
    "\n",
    "# --- Display metrics ---\n",
    "print(\"\\nüìä Logistic Regression Performance:\")\n",
    "\n",
    "print(f\"\\n   üîÑ Cross-Validation Results ({cv.n_splits} folds):\")\n",
    "for metric, (mean, std) in cv_results.items():\n",
    "    print(f\"      {metric}: {mean:.4f} ¬± {std:.4f}\")\n",
    "\n",
    "print(f\"\\n   Training Set (Full Fit):\")\n",
    "print(f\"      Accuracy:  {accuracy_train_lr:.4f}\")\n",
    "print(f\"      Precision: {precision_train_lr:.4f}\")\n",
    "print(f\"      Recall:    {recall_train_lr:.4f}\")\n",
    "print(f\"      F1-Score:  {f1_train_lr:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test Set:\")\n",
    "print(f\"      Accuracy:  {accuracy_test_lr:.4f}\")\n",
    "print(f\"      Precision: {precision_test_lr:.4f}\")\n",
    "print(f\"      Recall:    {recall_test_lr:.4f}\")\n",
    "print(f\"      F1-Score:  {f1_test_lr:.4f}\")\n",
    "\n",
    "# --- Overfitting check ---\n",
    "f1_diff_lr = f1_train_lr - f1_test_lr\n",
    "if abs(f1_diff_lr) < 0.01:\n",
    "    overfitting_status_lr = \"‚úÖ Excellent generalization\"\n",
    "elif f1_diff_lr > 0.05:\n",
    "    overfitting_status_lr = \"‚ö†Ô∏è Possible overfitting\"\n",
    "elif f1_diff_lr > 0.02:\n",
    "    overfitting_status_lr = \"‚ö° Minor overfitting\"\n",
    "else:\n",
    "    overfitting_status_lr = \"‚ÑπÔ∏è Normal variance\"\n",
    "\n",
    "print(f\"\\n   üîç Overfitting Analysis:\")\n",
    "print(f\"      Train-Test F1 diff: {f1_diff_lr:+.4f}\")\n",
    "print(f\"      Status: {overfitting_status_lr}\")\n",
    "\n",
    "# --- Comparison with baseline ---\n",
    "print(f\"\\nüÜö Comparison with best baseline (Test F1={f1_test_dummy:.4f}):\")\n",
    "improvement = f1_test_lr - f1_test_dummy\n",
    "if improvement > 0:\n",
    "    print(f\"   ‚úÖ Improvement: +{improvement:.4f} ({(improvement/f1_test_dummy)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Worse than baseline: {improvement:.4f}\")\n",
    "\n",
    "# --- Classification report ---\n",
    "print(f\"\\nüìã Classification Report - Logistic Regression:\")\n",
    "display(get_classification_report_table(y_test, y_pred_test_lr, \"Logistic Regression\"))\n",
    "\n",
    "# Importance of features\n",
    "\n",
    "print(\"\\nüìà Feature Importances (Logistic Regression Coefficients):\")\n",
    "\n",
    "# R√©cup√©ration du scaler et du mod√®le depuis le pipeline\n",
    "scaler = lr_pipeline.named_steps[\"scaler\"]\n",
    "model = lr_pipeline.named_steps[\"classifier\"]\n",
    "\n",
    "# R√©cup√©ration du nom des features (avec les transformations si besoin)\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# R√©cup√©ration des coefficients\n",
    "coefs = pd.Series(model.coef_[0], index=feature_names)\n",
    "\n",
    "# Calcul de l‚Äôimportance absolue\n",
    "importance_abs = coefs.abs().sort_values(ascending=False)\n",
    "\n",
    "# Top 15 plus influentes\n",
    "top_features = importance_abs.head(15)\n",
    "print(\"\\nTop 15 Features (by absolute coefficient value):\")\n",
    "print(top_features)\n",
    "\n",
    "# Ajout du signe pour interpr√©tation directionnelle\n",
    "coef_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": coefs.index,\n",
    "        \"Coefficient\": coefs.values,\n",
    "        \"Abs_Importance\": coefs.abs().values,\n",
    "    }\n",
    ").sort_values(by=\"Abs_Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nüîç Full coefficient summary (sorted):\")\n",
    "display(coef_summary.head(20))\n",
    "\n",
    "\n",
    "# --- Save pipeline & model ---\n",
    "lr_model_pipeline = lr_pipeline\n",
    "lr_model = lr_pipeline.named_steps['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variables for later comparison (keeping original names for compatibility)\n",
    "y_pred_lr = y_pred_test_lr\n",
    "accuracy_lr = accuracy_test_lr\n",
    "precision_lr = precision_test_lr\n",
    "recall_lr = recall_test_lr\n",
    "f1_lr = f1_test_lr\n",
    "\n",
    "print(f\"\\n‚úÖ Logistic Regression Pipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_analysis(y_test, y_pred_lr, model_name=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 5. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# RANDOM FOREST MODEL WITH CROSS-VALIDATION\n",
    "# ----------------------------------------\n",
    "print(\"RANDOM FOREST MODEL:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# --- Create Random Forest pipeline ---\n",
    "rf_pipeline = Pipeline(steps=[(\"classifier\", RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "))])\n",
    "\n",
    "# --- Cross-validation BEFORE fitting (methodologically correct) ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring_metrics = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": f\"precision_{AVERAGE_TYPE}\",\n",
    "    \"recall\": f\"recall_{AVERAGE_TYPE}\",\n",
    "    \"f1\": f\"f1_{AVERAGE_TYPE}\",\n",
    "}\n",
    "\n",
    "cv_results_rf = {}\n",
    "for metric_name, scoring in scoring_metrics.items():\n",
    "    scores = cross_val_score(\n",
    "        rf_pipeline, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1\n",
    "    )\n",
    "    cv_results_rf[metric_name] = (scores.mean(), scores.std())\n",
    "\n",
    "# --- Now fit the model for train/test evaluation ---\n",
    "rf_model = rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Predictions ---\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "# --- Metrics on train set ---\n",
    "accuracy_train_rf = accuracy_score(y_train, y_pred_train_rf)\n",
    "precision_train_rf = precision_score(\n",
    "    y_train, y_pred_train_rf, average=AVERAGE_TYPE, zero_division=0\n",
    ")\n",
    "recall_train_rf = recall_score(y_train, y_pred_train_rf, average=AVERAGE_TYPE)\n",
    "f1_train_rf = f1_score(y_train, y_pred_train_rf, average=AVERAGE_TYPE)\n",
    "\n",
    "# --- Metrics on test set ---\n",
    "accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)\n",
    "precision_test_rf = precision_score(\n",
    "    y_test, y_pred_test_rf, average=AVERAGE_TYPE, zero_division=0\n",
    ")\n",
    "recall_test_rf = recall_score(y_test, y_pred_test_rf, average=AVERAGE_TYPE)\n",
    "f1_test_rf = f1_score(y_test, y_pred_test_rf, average=AVERAGE_TYPE)\n",
    "# --- Display metrics ---\n",
    "print(f\"\\nüìä Random Forest Performance:\")\n",
    "\n",
    "print(f\"\\n   üîÑ Cross-Validation (5 folds, train set):\")\n",
    "for metric, (mean, std) in cv_results_rf.items():\n",
    "    print(f\"      {metric}: {mean:.4f} ¬± {std:.4f}\")\n",
    "\n",
    "print(f\"\\n   Training Set:\")\n",
    "print(f\"      Accuracy:  {accuracy_train_rf:.4f}\")\n",
    "print(f\"      Precision: {precision_train_rf:.4f}\")\n",
    "print(f\"      Recall:    {recall_train_rf:.4f}\")\n",
    "print(f\"      F1-Score:  {f1_train_rf:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test Set:\")\n",
    "print(f\"      Accuracy:  {accuracy_test_rf:.4f}\")\n",
    "print(f\"      Precision: {precision_test_rf:.4f}\")\n",
    "print(f\"      Recall:    {recall_test_rf:.4f}\")\n",
    "print(f\"      F1-Score:  {f1_test_rf:.4f}\")\n",
    "\n",
    "# --- Overfitting check ---\n",
    "f1_diff_rf = f1_train_rf - f1_test_rf\n",
    "if abs(f1_diff_rf) < 0.01:\n",
    "    overfitting_status_rf = \"‚úÖ Excellent generalization\"\n",
    "elif f1_diff_rf > 0.10:\n",
    "    overfitting_status_rf = \"üö® Significant overfitting\"\n",
    "elif f1_diff_rf > 0.05:\n",
    "    overfitting_status_rf = \"‚ö†Ô∏è Moderate overfitting\"\n",
    "elif f1_diff_rf > 0.02:\n",
    "    overfitting_status_rf = \"‚ö° Minor overfitting\"\n",
    "else:\n",
    "    overfitting_status_rf = \"‚ÑπÔ∏è Normal variance\"\n",
    "\n",
    "print(f\"\\n   üîç Overfitting Analysis:\")\n",
    "print(f\"      Train-Test F1 diff: {f1_diff_rf:+.4f}\")\n",
    "print(f\"      Status: {overfitting_status_rf}\")\n",
    "\n",
    "if f1_diff_rf > 0.05:\n",
    "    print(f\"      üí° Consider: Reduce max_depth, increase min_samples_split/leaf\")\n",
    "\n",
    "# --- Comparison with baseline ---\n",
    "print(f\"\\nüÜö Comparison with best baseline (Test F1={best_f1:.4f}):\")\n",
    "improvement_rf = f1_test_rf - best_f1\n",
    "if improvement_rf > 0:\n",
    "    print(\n",
    "        f\"   ‚úÖ Improvement: +{improvement_rf:.4f} ({(improvement_rf/best_f1)*100:.1f}%)\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"   ‚ùå Worse than baseline: {improvement_rf:.4f}\")\n",
    "\n",
    "# --- Classification report ---\n",
    "print(f\"\\nüìã Classification Report - Random Forest:\")\n",
    "display(get_classification_report_table(y_test, y_pred_test_rf, \"Random Forest\"))\n",
    "\n",
    "# --- Feature importance ---\n",
    "print(f\"\\nüîç Top 10 Most Important Features:\")\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"feature\": X.columns, \"importance\": rf_model.named_steps[\"classifier\"].feature_importances_}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "    print(f\"   {i+1:2d}. {row['feature']:<25} : {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest training completed!\")\n",
    "\n",
    "# --- Save model ---\n",
    "rf_model_pipeline = rf_model  # Pour compatibilit√© future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_analysis(y_test, y_pred_test_rf, model_name=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variables for later comparison (keeping original names for compatibility)\n",
    "y_pred_rf = y_pred_test_rf\n",
    "accuracy_rf = accuracy_test_rf\n",
    "precision_rf = precision_test_rf\n",
    "recall_rf = recall_test_rf\n",
    "f1_rf = f1_test_rf\n",
    "\n",
    "print(f\"\\n‚úÖ Random Forest Pipeline completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 6. XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# XGBOOST MODEL WITH CROSS-VALIDATION\n",
    "# ----------------------------------------\n",
    "print(\"XGBOOST MODEL:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# --- Create XGBoost pipeline ---\n",
    "xgb_pipeline = Pipeline(steps=[(\"classifier\", XGBClassifier(\n",
    "\tn_estimators=200,\n",
    "\tmax_depth=6,\n",
    "\tlearning_rate=0.1,\n",
    "\tsubsample=0.8,\n",
    "\tcolsample_bytree=0.8,\n",
    "\tmin_child_weight=4,\n",
    "\tscale_pos_weight=1,\n",
    "\trandom_state=42,\n",
    "\teval_metric=\"logloss\",\n",
    "\tn_jobs=-1,\n",
    "))])\n",
    "\n",
    "# --- Cross-validation BEFORE fitting ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring_metrics = {\n",
    "\t\"accuracy\": \"accuracy\",\n",
    "\t\"precision\": \"precision_weighted\",\n",
    "\t\"recall\": \"recall_weighted\",\n",
    "\t\"f1\": \"f1_weighted\",\n",
    "}\n",
    "\n",
    "cv_results_xgb = {}\n",
    "for metric_name, scoring in scoring_metrics.items():\n",
    "\tscores = cross_val_score(\n",
    "\t\txgb_pipeline, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1\n",
    "\t)\n",
    "\tcv_results_xgb[metric_name] = (scores.mean(), scores.std())\n",
    "\n",
    "# --- Fit the model for train/test evaluation ---\n",
    "xgb_model = xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Predictions ---\n",
    "y_pred_train_xgb = xgb_model.predict(X_train)\n",
    "y_pred_test_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)\n",
    "\n",
    "# --- Metrics on train set ---\n",
    "accuracy_train_xgb = accuracy_score(y_train, y_pred_train_xgb)\n",
    "precision_train_xgb = precision_score(\n",
    "\ty_train, y_pred_train_xgb, average=AVERAGE_TYPE, zero_division=0\n",
    ")\n",
    "recall_train_xgb = recall_score(y_train, y_pred_train_xgb, average=AVERAGE_TYPE)\n",
    "f1_train_xgb = f1_score(y_train, y_pred_train_xgb, average=AVERAGE_TYPE)\n",
    "\n",
    "# --- Metrics on test set ---\n",
    "accuracy_test_xgb = accuracy_score(y_test, y_pred_test_xgb)\n",
    "precision_test_xgb = precision_score(\n",
    "\ty_test, y_pred_test_xgb, average=AVERAGE_TYPE, zero_division=0\n",
    ")\n",
    "recall_test_xgb = recall_score(y_test, y_pred_test_xgb, average=AVERAGE_TYPE)\n",
    "f1_test_xgb = f1_score(y_test, y_pred_test_xgb, average=AVERAGE_TYPE)\n",
    "\n",
    "# --- Display metrics ---\n",
    "print(f\"\\nüìä XGBoost Performance:\")\n",
    "\n",
    "print(f\"\\n   üîÑ Cross-Validation (5 folds, train set):\")\n",
    "for metric, (mean, std) in cv_results_xgb.items():\n",
    "\tprint(f\"      {metric}: {mean:.4f} ¬± {std:.4f}\")\n",
    "\n",
    "print(f\"\\n   Training Set:\")\n",
    "print(f\"      Accuracy:  {accuracy_train_xgb:.4f}\")\n",
    "print(f\"      Precision: {precision_train_xgb:.4f}\")\n",
    "print(f\"      Recall:    {recall_train_xgb:.4f}\")\n",
    "print(f\"      F1-Score:  {f1_train_xgb:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test Set:\")\n",
    "print(f\"      Accuracy:  {accuracy_test_xgb:.4f}\")\n",
    "print(f\"      Precision: {precision_test_xgb:.4f}\")\n",
    "print(f\"      Recall:    {recall_test_xgb:.4f}\")\n",
    "print(f\"      F1-Score:  {f1_test_xgb:.4f}\")\n",
    "\n",
    "# --- Overfitting check ---\n",
    "f1_diff_xgb = f1_train_xgb - f1_test_xgb\n",
    "if abs(f1_diff_xgb) < 0.01:\n",
    "\toverfitting_status_xgb = \"‚úÖ Excellent generalization\"\n",
    "elif f1_diff_xgb > 0.10:\n",
    "\toverfitting_status_xgb = \"üö® Significant overfitting\"\n",
    "elif f1_diff_xgb > 0.05:\n",
    "\toverfitting_status_xgb = \"‚ö†Ô∏è Moderate overfitting\"\n",
    "elif f1_diff_xgb > 0.02:\n",
    "\toverfitting_status_xgb = \"‚ö° Minor overfitting\"\n",
    "else:\n",
    "\toverfitting_status_xgb = \"‚ÑπÔ∏è Normal variance\"\n",
    "\n",
    "print(f\"\\n   üîç Overfitting Analysis:\")\n",
    "print(f\"      Train-Test F1 diff: {f1_diff_xgb:+.4f}\")\n",
    "print(f\"      Status: {overfitting_status_xgb}\")\n",
    "\n",
    "if f1_diff_xgb > 0.05:\n",
    "\tprint(f\"      üí° Consider: Reduce max_depth, increase min_child_weight or regularization\")\n",
    "\n",
    "# --- Comparison with baseline ---\n",
    "print(f\"\\nüÜö Comparison with best baseline (Test F1={best_f1:.4f}):\")\n",
    "improvement_xgb = f1_test_xgb - best_f1\n",
    "if improvement_xgb > 0:\n",
    "\tprint(\n",
    "\t\tf\"   ‚úÖ Improvement: +{improvement_xgb:.4f} ({(improvement_xgb/best_f1)*100:.1f}%)\"\n",
    "\t)\n",
    "else:\n",
    "\tprint(f\"   ‚ùå Worse than baseline: {improvement_xgb:.4f}\")\n",
    "\n",
    "# --- Classification report ---\n",
    "print(f\"\\nüìã Classification Report - XGBoost:\")\n",
    "display(get_classification_report_table(y_test, y_pred_test_xgb, \"XGBoost\"))\n",
    "\n",
    "# --- Feature importance ---\n",
    "print(f\"\\nüîç Top 10 Most Important Features (XGBoost):\")\n",
    "feature_importance_xgb = pd.DataFrame(\n",
    "\t{\"feature\": X.columns, \"importance\": xgb_model.named_steps[\"classifier\"].feature_importances_}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "for i, (_, row) in enumerate(feature_importance_xgb.head(10).iterrows()):\n",
    "\tprint(f\"   {i+1:2d}. {row['feature']:<25} : {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nüåü XGBoost training completed!\")\n",
    "\n",
    "# --- Save model ---\n",
    "xgb_model_pipeline = xgb_model\n",
    "\n",
    "# Store variables for later comparison (keeping original names for compatibility)\n",
    "y_pred_xgb = y_pred_test_xgb\n",
    "accuracy_xgb = accuracy_test_xgb\n",
    "precision_xgb = precision_test_xgb\n",
    "recall_xgb = recall_test_xgb\n",
    "f1_xgb = f1_test_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# COMPREHENSIVE MODEL COMPARISON WITH CV RESULTS\n",
    "# ================================================\n",
    "print(\"MODEL COMPARISON AND SELECTION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare comprehensive comparison data\n",
    "models_summary = {\n",
    "    \"DummyClassifier (Baseline)\": {\n",
    "        \"cv\": dummy_results[best_strategy][\"cv\"],\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_test_dummy,\n",
    "            \"precision\": precision_test_dummy,\n",
    "            \"recall\": recall_test_dummy,\n",
    "            \"f1\": f1_test_dummy,\n",
    "        },\n",
    "        \"strategy\": best_strategy,\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"cv\": cv_results,\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_test_lr,\n",
    "            \"precision\": precision_test_lr,\n",
    "            \"recall\": recall_test_lr,\n",
    "            \"f1\": f1_test_lr,\n",
    "        },\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"cv\": cv_results_rf,\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_test_rf,\n",
    "            \"precision\": precision_test_rf,\n",
    "            \"recall\": recall_test_rf,\n",
    "            \"f1\": f1_test_rf,\n",
    "        },\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"cv\": cv_results_xgb,\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_test_xgb,\n",
    "            \"precision\": precision_test_xgb,\n",
    "            \"recall\": recall_test_xgb,\n",
    "            \"f1\": f1_test_xgb,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# ================================================\n",
    "# DETAILED COMPARISON TABLE\n",
    "# ================================================\n",
    "print(\"\\nüìä DETAILED MODEL COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<20} {'CV F1':<15}  {'Test F1':<16}  {'CV Stability':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "model_rankings = []\n",
    "\n",
    "for model_name, results in models_summary.items():\n",
    "    # Now we can use the proper f1 key\n",
    "    cv_f1 = results['cv'].get('f1', (0, 0))\n",
    "    test_f1 = results['test'].get('f1', results['test'].get('f1', 0))\n",
    "    \n",
    "    cv_stability = cv_f1[1] if isinstance(cv_f1, tuple) else 0\n",
    "    \n",
    "    # Stability assessment\n",
    "    if cv_stability < 0.01:\n",
    "        stability_emoji = \"‚úÖ\"\n",
    "    elif cv_stability < 0.02:\n",
    "        stability_emoji = \"üëç\"\n",
    "    elif cv_stability < 0.05:\n",
    "        stability_emoji = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        stability_emoji = \"üö®\"\n",
    "    \n",
    "    print(f\"{model_name:<20} \"\n",
    "          f\"{cv_f1[0] if isinstance(cv_f1, tuple) else cv_f1:<15.4f} \"\n",
    "          f\"{test_f1:<16.4f} \"\n",
    "          f\"{stability_emoji} {cv_stability:<10.4f}\")\n",
    "    \n",
    "    # Store for ranking\n",
    "    model_rankings.append({\n",
    "        'name': model_name,\n",
    "        'cv_f1': cv_f1[0] if isinstance(cv_f1, tuple) else cv_f1,\n",
    "        'test_f1': test_f1,\n",
    "        'stability': cv_stability\n",
    "    })\n",
    "\n",
    "# ================================================\n",
    "# MODEL RANKING AND SELECTION\n",
    "# ================================================\n",
    "print(f\"\\nüèÜ MODEL RANKING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Rank by test F1-weighted (primary metric)\n",
    "model_rankings.sort(key=lambda x: x['test_f1'], reverse=True)\n",
    "\n",
    "print(\"üìà By Test F1 Score:\")\n",
    "for i, model in enumerate(model_rankings, 1):\n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
    "    print(f\"   {medal} {model['name']:<25}: {model['test_f1']:.4f}\")\n",
    "\n",
    "# Rank by CV F1-weighted (reliability check)\n",
    "model_rankings_cv = sorted(model_rankings, key=lambda x: x['cv_f1'], reverse=True)\n",
    "print(\"\\nüîÑ By CV F1 Score (Training Reliability):\")\n",
    "for i, model in enumerate(model_rankings_cv, 1):\n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
    "    print(f\"   {medal} {model['name']:<25}: {model['cv_f1']:.4f}\")\n",
    "\n",
    "# ================================================\n",
    "# BEST MODEL SELECTION\n",
    "# ================================================\n",
    "best_model = model_rankings[0]  # Best by test F1-weighted\n",
    "best_model_name = best_model['name']\n",
    "\n",
    "print(f\"\\nüéØ FINAL MODEL SELECTION:\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   Test F1: {best_model['test_f1']:.4f}\")\n",
    "print(f\"   CV F1: {best_model['cv_f1']:.4f}\")\n",
    "print(f\"   CV Stability (std): {best_model['stability']:.4f}\")\n",
    "\n",
    "# Performance improvement over baseline\n",
    "baseline_perf = [m for m in model_rankings if 'Dummy' in m['name']][0]['test_f1']\n",
    "improvement = best_model['test_f1'] - baseline_perf\n",
    "\n",
    "if best_model['test_f1'] > baseline_perf:\n",
    "    if baseline_perf > 0:\n",
    "        improvement_pct = (improvement / baseline_perf) * 100\n",
    "        print(f\"   üìà Improvement over baseline: +{improvement:.4f} ({improvement_pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   üìà Improvement over baseline: +{improvement:.4f} (baseline was 0.0000)\")\n",
    "elif improvement == 0:\n",
    "    print(f\"   üü∞ Same performance as baseline: {best_model['test_']:.4f}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Performance issue: Model performs worse than baseline by {abs(improvement):.4f}!\")\n",
    "\n",
    "# ================================================\n",
    "# OVERFITTING ANALYSIS\n",
    "# ================================================\n",
    "print(f\"\\nüîç OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for model in model_rankings:\n",
    "    cv_test_diff = model['cv_f1'] - model['test_f1']\n",
    "    \n",
    "    if abs(cv_test_diff) < 0.01:\n",
    "        status = \"‚úÖ Excellent generalization\"\n",
    "    elif cv_test_diff > 0.05:\n",
    "        status = \"‚ö†Ô∏è Possible overfitting\"\n",
    "    elif cv_test_diff > 0.02:\n",
    "        status = \"‚ö° Minor overfitting\"\n",
    "    elif cv_test_diff < -0.02:\n",
    "        status = \"üìâ Underfitting\"\n",
    "    else:\n",
    "        status = \"üëç Good generalization\"\n",
    "    \n",
    "    print(f\"{model['name']:<25}: CV-Test diff = {cv_test_diff:+.4f} ‚Üí {status}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model comparison completed!\")\n",
    "print(f\"üéØ Recommended model: {best_model_name}\")\n",
    "\n",
    "# Store results for potential further use\n",
    "comparison_results = {\n",
    "    'best_model': best_model_name,\n",
    "    'rankings': model_rankings,\n",
    "    'summary': models_summary\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 7. Detailed Performance Analysis by Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# DETAILED PERFORMANCE ANALYSIS BY CLASS\n",
    "# ================================================\n",
    "print(\"DETAILED PERFORMANCE ANALYSIS BY CLASS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect predictions from all models\n",
    "model_predictions = {\n",
    "    'DummyClassifier (Baseline)': y_pred_test_dummy,\n",
    "    'Logistic Regression': y_pred_test_lr,\n",
    "    'Random Forest': y_pred_test_rf,\n",
    "    'XGBoost': y_pred_test_xgb\n",
    "}\n",
    "\n",
    "# Class labels mapping\n",
    "class_labels = {0: \"Stayed (Class 0)\", 1: \"Left (Class 1)\"}\n",
    "\n",
    "print(f\"\\nüìä CLASS DISTRIBUTION IN TEST SET:\")\n",
    "print(\"-\" * 40)\n",
    "test_class_counts = y_test.value_counts().sort_index()\n",
    "for class_val, count in test_class_counts.items():\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"   {class_labels[class_val]:<20}: {count:3d} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# ================================================\n",
    "# PER-CLASS METRICS FOR EACH MODEL\n",
    "# ================================================\n",
    "print(f\"\\nüìà PER-CLASS PERFORMANCE METRICS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, y_pred in model_predictions.items():\n",
    "    print(f\"\\nüî∏ {model_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Display metrics for each class\n",
    "    for class_val in [0, 1]:\n",
    "        class_str = str(class_val)\n",
    "        if class_str in report:\n",
    "            metrics = report[class_str]\n",
    "            print(f\"\\n   {class_labels[class_val]}:\")\n",
    "            print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"      Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"      F1-Score:  {metrics['f1-score']:.4f}\")\n",
    "            print(f\"      Support:   {metrics['support']:.0f} samples\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"\\n   üìä Overall Performance:\")\n",
    "    print(f\"      Accuracy:     {report['accuracy']:.4f}\")\n",
    "    print(f\"      Macro Avg F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"      Weighted F1:  {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# ================================================\n",
    "# CLASS-SPECIFIC COMPARISON TABLE\n",
    "# ================================================\n",
    "print(f\"\\nüîç CLASS-SPECIFIC COMPARISON ACROSS MODELS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison table for each class\n",
    "for class_val in [0, 1]:\n",
    "    print(f\"\\nüìä {class_labels[class_val]} Performance:\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'Model':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    class_comparisons = []\n",
    "    \n",
    "    for model_name, y_pred in model_predictions.items():\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        class_str = str(class_val)\n",
    "        \n",
    "        if class_str in report:\n",
    "            metrics = report[class_str]\n",
    "            precision = metrics['precision']\n",
    "            recall = metrics['recall']\n",
    "            f1 = metrics['f1-score']\n",
    "            \n",
    "            print(f\"{model_name:<25} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f}\")\n",
    "            \n",
    "            class_comparisons.append({\n",
    "                'model': model_name,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            })\n",
    "    \n",
    "    # Find best model for this class\n",
    "    if class_comparisons:\n",
    "        best_f1_model = max(class_comparisons, key=lambda x: x['f1'])\n",
    "        best_recall_model = max(class_comparisons, key=lambda x: x['recall'])\n",
    "        \n",
    "        print(f\"\\n   üèÜ Best F1-Score:  {best_f1_model['model']} ({best_f1_model['f1']:.4f})\")\n",
    "        print(f\"   üéØ Best Recall:    {best_recall_model['model']} ({best_recall_model['recall']:.4f})\")\n",
    "\n",
    "# ================================================\n",
    "# CONFUSION MATRIX COMPARISON\n",
    "# ================================================\n",
    "print(f\"\\nüîç CONFUSION MATRIX COMPARISON:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, y_pred in model_predictions.items():\n",
    "    print(f\"\\nüî∏ {model_name}:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(\"   Confusion Matrix:\")\n",
    "    print(f\"               Predicted\")\n",
    "    print(f\"             Stayed  Left\")\n",
    "    print(f\"   Actual Stayed  {cm[0,0]:3d}   {cm[0,1]:3d}\")\n",
    "    print(f\"          Left    {cm[1,0]:3d}   {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    if len(cm) == 2 and cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        # Calculate rates\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate (Recall for Class 1)\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate (Recall for Class 0)\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        print(f\"\\n   üìä Key Rates:\")\n",
    "        print(f\"      True Positive Rate (Sensitivity):  {tpr:.4f}\")\n",
    "        print(f\"      True Negative Rate (Specificity):  {tnr:.4f}\")\n",
    "        print(f\"      False Positive Rate:               {fpr:.4f}\")\n",
    "        print(f\"      False Negative Rate:               {fnr:.4f}\")\n",
    "\n",
    "# ================================================\n",
    "# BUSINESS IMPACT ANALYSIS\n",
    "# ================================================\n",
    "print(f\"\\nüíº BUSINESS IMPACT ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nüìà Key Business Metrics:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for model_name, y_pred in model_predictions.items():\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    if len(cm) == 2 and cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        # Business interpretations\n",
    "        correctly_identified_leavers = tp\n",
    "        missed_leavers = fn\n",
    "        false_alarms = fp\n",
    "        correctly_identified_stayers = tn\n",
    "        \n",
    "        total_actual_leavers = tp + fn\n",
    "        total_actual_stayers = tn + fp\n",
    "        \n",
    "        print(f\"\\nüî∏ {model_name}:\")\n",
    "        print(f\"   Correctly identified employees who left:  {correctly_identified_leavers}/{total_actual_leavers} ({(correctly_identified_leavers/total_actual_leavers)*100:.1f}%)\")\n",
    "        print(f\"   Missed employees who left:                {missed_leavers}/{total_actual_leavers} ({(missed_leavers/total_actual_leavers)*100:.1f}%)\")\n",
    "        print(f\"   False alarms (predicted left but stayed): {false_alarms}/{total_actual_stayers} ({(false_alarms/total_actual_stayers)*100:.1f}%)\")\n",
    "        \n",
    "        # Cost-benefit consideration\n",
    "        if total_actual_leavers > 0:\n",
    "            prevention_success_rate = correctly_identified_leavers / total_actual_leavers\n",
    "            print(f\"   Potential intervention success rate:       {prevention_success_rate:.1%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Detailed class-specific analysis completed!\")\n",
    "print(f\"üí° Use these insights to understand which employees your model identifies best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Precision Recall curve for Logistic Regression, Random Forest, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionnaire des mod√®les et leurs probabilit√©s pour la classe positive\n",
    "models_proba = {\n",
    "    \"DummyClassifier (Baseline)\": y_pred_proba_dummy,\n",
    "    \"Logistic Regression\": y_pred_proba_lr,\n",
    "    \"Random Forest\": y_pred_proba_rf,\n",
    "    \"XGBoost\": y_pred_proba_xgb,\n",
    "}\n",
    "\n",
    "model_colors = {\n",
    "    \"DummyClassifier (Baseline)\": \"gray\",\n",
    "    \"Logistic Regression\": \"blue\",\n",
    "    \"Random Forest\": \"green\",\n",
    "    \"XGBoost\": \"orange\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for model_name, y_pred_proba in models_proba.items():\n",
    "    # S√©lection de la probabilit√© de la classe positive\n",
    "    y_scores = y_pred_proba[:, 1] if y_pred_proba.ndim > 1 else y_pred_proba\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "    average_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "    plt.plot(\n",
    "        recall,\n",
    "        precision,\n",
    "        label=f\"{model_name} (AP = {average_precision:.2f})\",\n",
    "        color=model_colors.get(model_name, None),\n",
    "    )\n",
    "\n",
    "# D√©corations du graphique\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === 1. Donn√©es de d√©part ===\n",
    "recap_data = {\n",
    "    \"Model\": [\n",
    "        \"DummyClassifier (Baseline)\",\n",
    "        \"Logistic Regression\",\n",
    "        \"Random Forest\",\n",
    "        \"XGBoost\",\n",
    "    ],\n",
    "    \"Accuracy Train\": [\n",
    "        accuracy_train_dummy,\n",
    "        accuracy_train_lr,\n",
    "        accuracy_train_rf,\n",
    "        accuracy_train_xgb,\n",
    "    ],\n",
    "    \"Accuracy Test\": [\n",
    "        accuracy_test_dummy,\n",
    "        accuracy_test_lr,\n",
    "        accuracy_test_rf,\n",
    "        accuracy_test_xgb,\n",
    "    ],\n",
    "    \"Recall Train\": [\n",
    "        recall_train_dummy,\n",
    "        recall_train_lr,\n",
    "        recall_train_rf,\n",
    "        recall_train_xgb,\n",
    "    ],\n",
    "    \"Recall Test\": [recall_test_dummy, recall_test_lr, recall_test_rf, recall_test_xgb],\n",
    "    \"F1 Train\": [f1_train_dummy, f1_train_lr, f1_train_rf, f1_train_xgb],\n",
    "    \"F1 Test\": [f1_test_dummy, f1_test_lr, f1_test_rf, f1_test_xgb],\n",
    "    \"Overfitting (Train-Test F1)\": [\n",
    "        None,\n",
    "        f1_train_lr - f1_test_lr,\n",
    "        f1_train_rf - f1_test_rf,\n",
    "        f1_train_xgb - f1_test_xgb,\n",
    "    ],\n",
    "    \"Recall Class 1 (Test)\": [\n",
    "        classification_report(\n",
    "            y_test, y_pred_test_dummy, output_dict=True, zero_division=0\n",
    "        )[\"1\"][\"recall\"],\n",
    "        classification_report(\n",
    "            y_test, y_pred_test_lr, output_dict=True, zero_division=0\n",
    "        )[\"1\"][\"recall\"],\n",
    "        classification_report(\n",
    "            y_test, y_pred_test_rf, output_dict=True, zero_division=0\n",
    "        )[\"1\"][\"recall\"],\n",
    "        classification_report(\n",
    "            y_test, y_pred_test_xgb, output_dict=True, zero_division=0\n",
    "        )[\"1\"][\"recall\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "# === 2. Cr√©ation du DataFrame ===\n",
    "recap_df = pd.DataFrame(recap_data)\n",
    "\n",
    "# === 3. S√©lection des colonnes num√©riques √† visualiser ===\n",
    "metrics_cols = [\n",
    "    \"Accuracy Train\",\n",
    "    \"Accuracy Test\",\n",
    "    \"Recall Train\",\n",
    "    \"Recall Test\",\n",
    "    \"F1 Train\",\n",
    "    \"F1 Test\",\n",
    "    \"Overfitting (Train-Test F1)\",\n",
    "    \"Recall Class 1 (Test)\",\n",
    "]\n",
    "\n",
    "# === 4. Pr√©paration pour le heatmap ===\n",
    "heat_df = recap_df.set_index(\"Model\")[metrics_cols].astype(float)\n",
    "\n",
    "# === 5. Affichage avec Seaborn ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(\n",
    "    heat_df,\n",
    "    annot=True,  # affiche les valeurs\n",
    "    fmt=\".2f\",  # 2 d√©cimales\n",
    "    cmap=\"YlGnBu\",  # palette bleue/verte\n",
    "    linewidths=0.5,  # fines s√©parations\n",
    "    cbar=True,  # barre de couleur √† droite\n",
    ")\n",
    "plt.title(\"Model Performance Comparison\", fontsize=13, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p4-esn-technova-partners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

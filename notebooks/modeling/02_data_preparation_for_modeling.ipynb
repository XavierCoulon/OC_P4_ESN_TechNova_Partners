{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utils functions\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to Python path to import our utils module\n",
    "src_path = os.path.abspath('../../src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import feature engineering utilities from our custom module\n",
    "from utils import ( # type: ignore\n",
    "    identify_feature_types,\n",
    "    create_correlation_matrix,\n",
    "    apply_binary_encoding,\n",
    "    apply_onehot_encoding,\n",
    "    apply_ordinal_encoding,\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: TARGET VARIABLE\n",
    "# ============================================================================\n",
    "TARGET_VARIABLE = 'a_quitte_l_entreprise'  # Employee turnover prediction (0=stayed, 1=left)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üéØ Target variable set to: {TARGET_VARIABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load the Cleaned Dataset\n",
    "\n",
    "Load the merged and cleaned dataset from the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned merged dataset\n",
    "data_path = '../../data/processed/employee_data_merged_clean.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üîç Columns: {len(df.columns)} features\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.info())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {data_path}\")\n",
    "    print(\"Please ensure the EDA notebook has been run and the cleaned dataset exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Feature Type Analysis\n",
    "\n",
    "Analyze the dataset to identify different types of features automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if utils functions are available, if not provide a fallback\n",
    "try:\n",
    "    # Test if we can call a function from utils\n",
    "    print(\"FEATURE TYPE ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Identify different types of features in the dataset\n",
    "    feature_types = identify_feature_types(df, exclude_cols=[TARGET_VARIABLE])\n",
    "    \n",
    "    print(\"‚úÖ Feature type analysis complete!\")\n",
    "    print(f\"Feature type breakdown:\")\n",
    "    for feature_type, features in feature_types.items():\n",
    "        print(f\"  ‚Ä¢ {feature_type.replace('_', ' ').title()}: {len(features)} features\")\n",
    "        if features and len(features) <= 8:\n",
    "            print(f\"    - {', '.join(features)}\")\n",
    "        elif features:\n",
    "            print(f\"    - {', '.join(features[:5])} ... and {len(features)-5} more\")\n",
    "    \n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è Utils functions not available. Please run the import cell first.\")\n",
    "    print(\"If the import fails, you may need to:\")\n",
    "    print(\"1. Check that src/utils.py exists\")\n",
    "    print(\"2. Restart the kernel and run cells in order\")\n",
    "    print(\"3. Use absolute import path\")\n",
    "    \n",
    "    # Create a basic feature_types dictionary as fallback\n",
    "    feature_types = {\n",
    "        'numerical_continuous': [],\n",
    "        'numerical_discrete': [],\n",
    "        'categorical_ordinal': [],\n",
    "        'categorical_nominal': [],\n",
    "        'binary': [],\n",
    "        'id_columns': []\n",
    "    }\n",
    "    \n",
    "    # Manual feature type identification as fallback\n",
    "    for col in df.columns:\n",
    "        if col == TARGET_VARIABLE:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].nunique() == 2:\n",
    "                feature_types['binary'].append(col)\n",
    "            elif df[col].nunique() <= 10:\n",
    "                feature_types['categorical_ordinal'].append(col)\n",
    "            else:\n",
    "                feature_types['numerical_continuous'].append(col)\n",
    "        else:\n",
    "            feature_types['categorical_nominal'].append(col)\n",
    "    \n",
    "    print(\"üìã Fallback feature type analysis:\")\n",
    "    for feature_type, features in feature_types.items():\n",
    "        if features:\n",
    "            print(f\"  ‚Ä¢ {feature_type.replace('_', ' ').title()}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Feature Encoding and Preprocessing\n",
    "\n",
    "Apply appropriate encoding methods for categorical features and prepare data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features for encoding\n",
    "categorical_features = df.select_dtypes(exclude=['number', 'boolean']).columns.tolist()\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'departement' column using our utils function\n",
    "one_hot_encoding_features = ['departement', 'statut_marital', 'poste']\n",
    "\n",
    "for feature in one_hot_encoding_features:\n",
    "\tdf, encoding_info = apply_onehot_encoding(df, [feature])\n",
    "\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding for 'genre' column\n",
    "df[\"genre\"].value_counts()\n",
    "df, encoding_info = apply_binary_encoding(df, ['genre'], {'genre': {'M': 0, 'F': 1}})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding for 'heure_supplementaires' column\n",
    "df[\"heure_supplementaires\"].value_counts()\n",
    "df, encoding_info = apply_binary_encoding(df, [\"heure_supplementaires\"], {\"heure_supplementaires\": {\"Non\": 0, \"Oui\": 1}})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding for 'a_quitte_l_entreprise' column\n",
    "df[TARGET_VARIABLE].value_counts()\n",
    "df, encoding_info = apply_binary_encoding(df, [TARGET_VARIABLE], {TARGET_VARIABLE: {\"Non\": 0, \"Oui\": 1}})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'augementation_salaire_precedente' from percentage string to float\n",
    "df[\"augementation_salaire_precedente\"].value_counts()\n",
    "df[\"augementation_salaire_precedente\"] = df[\"augementation_salaire_precedente\"].str.rstrip(\" %\").astype(float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'domaine_etude' column\n",
    "df[\"domaine_etude\"].value_counts()\n",
    "df, encoding_info = apply_onehot_encoding(\n",
    "    df, [\"domaine_etude\"]\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding for 'frequence_deplacement' column\n",
    "df[\"frequence_deplacement\"].value_counts()\n",
    "df, encoding_info = apply_ordinal_encoding(df, [\"frequence_deplacement\"], {\"frequence_deplacement\": [\"Aucun\", \"Occasionnel\", \"Frequent\"]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all categorical features have been encoded\n",
    "remaining_categorical_features = df.select_dtypes(exclude=['number', 'boolean']).columns.tolist()\n",
    "if not remaining_categorical_features:\n",
    "\tprint(\"‚úÖ All categorical features have been encoded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 5. Prepare Features and Target Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "print(\"PREPARING FINAL FEATURES AND TARGET:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Drop target and ID columns from features\n",
    "y = df[TARGET_VARIABLE].copy()\n",
    "X_temp = df.drop(columns=[TARGET_VARIABLE, \"id_employee\"])  \n",
    "\n",
    "\n",
    "print(f\"Features shape before correlation filtering: {X_temp.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target variable: {TARGET_VARIABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 5.1. Feature Engineering\n",
    "\n",
    "Apply feature engineering transformations to create new features and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with raw features.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mobilit√© interne ratio\n",
    "    epsilon = 1e-6\n",
    "    df[\"mobilite_interne_ratio\"] = df[\"annees_dans_le_poste_actuel\"] / (df[\"annees_dans_l_entreprise\"] + epsilon)\n",
    "    # df.drop(columns=[\"annees_dans_le_poste_actuel\"], inplace=True)\n",
    "\n",
    "    # Ratio anciennet√©\n",
    "    df[\"ratio_anciennete\"] = df[\"annees_dans_l_entreprise\"] / (df[\"annees_dans_l_entreprise\"] + df[\"nombre_experiences_precedentes\"] + epsilon)\n",
    "\n",
    "    # Evolution de la note d'√©valuation\n",
    "    df[\"delta_evaluation\"] = df[\"note_evaluation_actuelle\"] - df[\"note_evaluation_precedente\"]\n",
    "    df.drop(columns=[\"note_evaluation_precedente\"], inplace=True)\n",
    "\n",
    "    # Ecart salarial = revenu de l'employ√© / revenu moyen au m√™me poste pour le me\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"APPLYING FEATURE ENGINEERING:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Features shape before feature engineering: {X_temp.shape}\")\n",
    "X_temp = feature_engineering(X_temp)\n",
    "print(f\"Features shape after feature engineering: {X_temp.shape}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering completed!\")\n",
    "\n",
    "X_temp[\"ratio_anciennete\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis (Post Feature Engineering)\n",
    "\n",
    "Analyze correlations between the final engineered features (X only) to identify multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for correlation analysis (numerical features only)\n",
    "print(\"FEATURE CORRELATION ANALYSIS ON FINAL X FEATURES:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get only numerical features from X for correlation analysis\n",
    "numerical_cols_in_X = [col for col in X_temp.columns if pd.api.types.is_numeric_dtype(X_temp[col])]\n",
    "print(f\"Analyzing correlations for {len(numerical_cols_in_X)} numerical features:\")\n",
    "\n",
    "# Show feature breakdown\n",
    "if len(numerical_cols_in_X) <= 15:\n",
    "    for col in numerical_cols_in_X:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    for col in numerical_cols_in_X[:10]:\n",
    "        print(f\"  - {col}\")\n",
    "    print(f\"  ... and {len(numerical_cols_in_X) - 10} more features\")\n",
    "\n",
    "print(f\"\\nNote: Excluding target variable '{TARGET_VARIABLE}' from correlation analysis\")\n",
    "print(f\"Features include encoded categorical variables (one-hot encoded columns)\")\n",
    "\n",
    "# Create subset for correlation analysis\n",
    "X_numerical_for_corr = X_temp[numerical_cols_in_X].copy()\n",
    "print(f\"\\nCorrelation analysis dataset shape: {X_numerical_for_corr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to potentially drop based on correlation analysis\n",
    "features_to_potentially_drop = [\n",
    "    # \"niveau_hierarchique_poste\", # Not really relevant in ESN context\n",
    "    # \"annees_dans_le_poste_actuel\",\n",
    "    # \"annes_sous_responsable_actuel\",\n",
    "    \"departement_Consulting\",\n",
    "    \"departement_Ressources Humaines\",\n",
    "]\n",
    "\n",
    "# Drop from X_numerical_for_corr\n",
    "X_numerical_for_corr = X_numerical_for_corr.drop(columns=features_to_potentially_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation analysis on final features\n",
    "print(\"SPEARMAN CORRELATION ANALYSIS (FINAL FEATURES):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "spearman_corr, spearman_high_corr = create_correlation_matrix(\n",
    "    X_numerical_for_corr, method='spearman', threshold=0.7\n",
    ")\n",
    "\n",
    "print(f\"Highly correlated feature pairs (|correlation| >= 0.7):\")\n",
    "if spearman_high_corr:\n",
    "    for pair in spearman_high_corr:\n",
    "        print(f\"  - {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"  - No highly correlated pairs found\")\n",
    "\n",
    "# Visualize Spearman correlation matrix\n",
    "plt.figure(figsize=(16, 12))\n",
    "mask = np.triu(np.ones_like(spearman_corr, dtype=bool))\n",
    "sns.heatmap(spearman_corr, mask=mask, annot=True, cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            fmt='.1f', annot_kws={'size': 8})\n",
    "plt.title('Spearman Correlation Matrix (Final Encoded Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Spearman correlation matrix shape: {spearman_corr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation analysis on final features\n",
    "print(\"PEARSON CORRELATION ANALYSIS (FINAL FEATURES):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pearson_corr, pearson_high_corr = create_correlation_matrix(\n",
    "    X_numerical_for_corr, method='pearson', threshold=0.7\n",
    ")\n",
    "\n",
    "print(f\"Highly correlated feature pairs (|correlation| >= 0.7):\")\n",
    "if pearson_high_corr:\n",
    "    for pair in pearson_high_corr:\n",
    "        print(f\"  - {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"  - No highly correlated pairs found\")\n",
    "\n",
    "# Visualize Pearson correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
    "sns.heatmap(pearson_corr, mask=mask, annot=True, cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            fmt='.2f', annot_kws={'size': 8})\n",
    "plt.title('Feature Correlation Matrix (Final Encoded Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Correlation matrix shape: {pearson_corr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All high correlation pairs from both analyses\n",
    "all_high_corr_pairs = spearman_high_corr + pearson_high_corr\n",
    "if all_high_corr_pairs:\n",
    "\tprint(f\"\\nTotal unique highly correlated feature pairs from both analyses: {len(all_high_corr_pairs)}\")\n",
    "\tfor pair in all_high_corr_pairs:\n",
    "\t\tprint(f\"  - {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined correlation analysis summary on engineered features\n",
    "print(\"FINAL CORRELATION ANALYSIS (POST FEATURE ENGINEERING):\")\n",
    "print(\"=\"*60)\n",
    "print(\"Analyzing correlations on the engineered feature set...\")\n",
    "\n",
    "numerical_cols_in_X_temp = [\n",
    "    col for col in X_temp.columns if pd.api.types.is_numeric_dtype(X_temp[col])\n",
    "]\n",
    "print(f\"Features after engineering: {len(numerical_cols_in_X_temp)} numerical features\")\n",
    "\n",
    "if len(numerical_cols_in_X_temp) > 1:\n",
    "    total_features = len(numerical_cols_in_X_temp)\n",
    "\n",
    "    # Combine high correlation pairs from both methods\n",
    "    all_high_corr_pairs = []\n",
    "    \n",
    "    # Add Spearman pairs if available\n",
    "    if 'spearman_high_corr' in locals() and spearman_high_corr:\n",
    "        for pair in spearman_high_corr:\n",
    "            pair['method'] = 'Spearman'\n",
    "            all_high_corr_pairs.append(pair)\n",
    "    \n",
    "    # Add Pearson pairs if available\n",
    "    if 'pearson_high_corr' in locals() and pearson_high_corr:\n",
    "        for pair in pearson_high_corr:\n",
    "            pair['method'] = 'Pearson'\n",
    "            all_high_corr_pairs.append(pair)\n",
    "    \n",
    "    # Remove duplicates (same feature pair found by both methods)\n",
    "    unique_pairs = {}\n",
    "    for pair in all_high_corr_pairs:\n",
    "        key = tuple(sorted([pair['feature1'], pair['feature2']]))\n",
    "        if key not in unique_pairs:\n",
    "            unique_pairs[key] = pair\n",
    "        else:\n",
    "            # Keep the one with higher absolute correlation\n",
    "            if abs(pair['correlation']) > abs(unique_pairs[key]['correlation']):\n",
    "                unique_pairs[key] = pair\n",
    "    \n",
    "    unique_high_corr_pairs = list(unique_pairs.values())\n",
    "    \n",
    "    print(f\"üìä Total features analyzed: {total_features}\")\n",
    "    print(f\"üîó High correlation pairs found (‚â•0.7):\")\n",
    "    print(f\"   - Spearman method: {len([p for p in all_high_corr_pairs if p.get('method') == 'Spearman'])}\")\n",
    "    print(f\"   - Pearson method: {len([p for p in all_high_corr_pairs if p.get('method') == 'Pearson'])}\")\n",
    "    print(f\"   - Unique pairs (combined): {len(unique_high_corr_pairs)}\")\n",
    "    \n",
    "    if unique_high_corr_pairs:\n",
    "        print(f\"\\n‚ö†Ô∏è  MULTICOLLINEARITY DETECTED:\")\n",
    "        print(f\"   Found {len(unique_high_corr_pairs)} unique pairs of highly correlated features\")\n",
    "        print(f\"   These may cause issues in linear models\")\n",
    "        print(f\"   Consider manually removing one feature from each pair\")\n",
    "        \n",
    "        print(f\"\\nüìã Highly correlated feature pairs:\")\n",
    "        for pair in unique_high_corr_pairs[:10]:  # Show top 10\n",
    "            print(f\"   - {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f} ({pair['method']})\")\n",
    "        \n",
    "        if len(unique_high_corr_pairs) > 10:\n",
    "            print(f\"   ... and {len(unique_high_corr_pairs) - 10} more pairs\")\n",
    "        \n",
    "        # Show which features appear most frequently in correlations\n",
    "        feature_counts = {}\n",
    "        for pair in unique_high_corr_pairs:\n",
    "            feature_counts[pair['feature1']] = feature_counts.get(pair['feature1'], 0) + 1\n",
    "            feature_counts[pair['feature2']] = feature_counts.get(pair['feature2'], 0) + 1\n",
    "        \n",
    "        if feature_counts:\n",
    "            print(f\"\\nüéØ Features involved in multiple correlations:\")\n",
    "            sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            for feature, count in sorted_features[:5]:  # Top 5\n",
    "                print(f\"   - {feature}: {count} correlation(s)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ NO MULTICOLLINEARITY ISSUES:\")\n",
    "        print(f\"   All feature correlations are below 0.7 threshold\")\n",
    "        print(f\"   Features are suitable for linear models\")\n",
    "        \n",
    "    # Calculate max correlation from both methods\n",
    "    max_corrs = []\n",
    "    if 'spearman_corr' in locals():\n",
    "        spearman_max = spearman_corr.abs().values\n",
    "        spearman_max = spearman_max[spearman_max < 1.0].max()\n",
    "        max_corrs.append(('Spearman', spearman_max))\n",
    "    \n",
    "    if 'pearson_corr' in locals():\n",
    "        pearson_max = pearson_corr.abs().values\n",
    "        pearson_max = pearson_max[pearson_max < 1.0].max()\n",
    "        max_corrs.append(('Pearson', pearson_max))\n",
    "    \n",
    "    if max_corrs:\n",
    "        print(f\"\\nüìà Maximum absolute correlations:\")\n",
    "        for method, max_corr in max_corrs:\n",
    "            print(f\"   - {method}: {max_corr:.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå Could not perform correlation analysis\")\n",
    "\n",
    "print(f\"\\nüí° Next step: Manually review and decide on feature selection if needed\")\n",
    "print(f\"   Both Spearman and Pearson analyses provide complementary insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final X and y for modeling\n",
    "print(\"PREPARING FINAL X AND Y:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use X_temp as final X (no automatic correlation removal)\n",
    "X = X_temp.copy()\n",
    "\n",
    "print(f\"Final dataset dimensions:\")\n",
    "print(f\"  - X (features): {X.shape}\")\n",
    "print(f\"  - y (target): {y.shape}\")\n",
    "print(f\"  - Total features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ X and y ready for modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 7. Export Processed Data\n",
    "\n",
    "Save the final preprocessed features (X) and target (y) for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data for modeling\n",
    "print(\"EXPORTING PROCESSED DATA:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create data folder for processed data\n",
    "import os\n",
    "processed_data_path = \"../../data/processed\"\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# Export features (X) and target (y)\n",
    "X_file = os.path.join(processed_data_path, \"X_features.csv\")\n",
    "y_file = os.path.join(processed_data_path, \"y_target.csv\")\n",
    "\n",
    "X.to_csv(X_file, index=False)\n",
    "y.to_csv(y_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Features saved to: {X_file}\")\n",
    "print(f\"   Shape: {X.shape}\")\n",
    "print(f\"   Columns: {list(X.columns)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Target saved to: {y_file}\")\n",
    "print(f\"   Shape: {y.shape}\")\n",
    "print(f\"   Target variable: {TARGET_VARIABLE}\")\n",
    "\n",
    "print(f\"\\nüìä Final dataset summary:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Total features: {X.shape[1]}\")\n",
    "print(f\"   Target classes: {sorted(y.unique())}\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for modeling in next notebook!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p4-esn-technova-partners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

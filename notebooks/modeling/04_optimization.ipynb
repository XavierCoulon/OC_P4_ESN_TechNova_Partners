{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "# Add the src directory to Python path to import our utils module\n",
    "src_path = os.path.abspath(\"../../src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import feature engineering utilities from our custom module\n",
    "from project_utils import (  # type: ignore\n",
    "    confusion_matrix_analysis,\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "\n",
    "# Styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"LOADING PREPROCESSED DATA:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define data paths\n",
    "data_path = \"../../data/processed\"\n",
    "X_file = os.path.join(data_path, \"X_features.csv\")\n",
    "y_file = os.path.join(data_path, \"y_target.csv\")\n",
    "\n",
    "# Load features and target\n",
    "X = pd.read_csv(X_file)\n",
    "y = pd.read_csv(y_file).squeeze()  # Convert to Series\n",
    "\n",
    "print(f\"‚úÖ Features loaded: {X.shape}\")\n",
    "print(f\"‚úÖ Target loaded: {y.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Total features: {X.shape[1]}\")\n",
    "print(f\"   Target variable: {y.name}\")\n",
    "print(f\"   Target classes: {sorted(y.unique())}\")\n",
    "print(f\"   Class distribution:\")\n",
    "for class_val in sorted(y.unique()):\n",
    "    count = (y == class_val).sum()\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"     Class {class_val}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Data ready for modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "print(\"TRAIN/TEST SPLIT:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train set: {X_train.shape}\")\n",
    "print(f\"‚úÖ Test set: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Class distribution after split:\")\n",
    "print(f\"\\nTraining set:\")\n",
    "for class_val in sorted(y_train.unique()):\n",
    "    count = (y_train == class_val).sum()\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  Class {class_val}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "for class_val in sorted(y_test.unique()):\n",
    "    count = (y_test == class_val).sum()\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  Class {class_val}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Stratified split completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================\n",
    "# Create Pipeline and CV Strategy\n",
    "# ================================\n",
    "# Create a pipeline with RandomForest\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ================================\n",
    "# Param√®tres √† tester pour GridSearch\n",
    "# ================================\n",
    "param_grid = {\n",
    "    \"rf__n_estimators\": [200, 300, 400],\n",
    "    \"rf__max_depth\": [6, 7, 8],\n",
    "    \"rf__min_samples_split\": [4, 8, 12],\n",
    "    \"rf__min_samples_leaf\": [2, 4, 6],\n",
    "    \"rf__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "print(\"üîç GridSearchCV Configuration:\")\n",
    "print(f\"   Pipeline steps: {list(pipeline.named_steps.keys())}\")\n",
    "print(f\"   CV strategy: {cv}\")\n",
    "print(f\"   Parameter combinations: {len(list(ParameterGrid(param_grid)))}\")\n",
    "print(f\"   Scoring metric: recall\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"recall\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Meilleurs param√®tres trouv√©s :\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Meilleur Recall (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\nüìã GridSearchCV termin√©. √âvaluation finale avec seuil optimal dans la cellule suivante...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# SEUIL OPTIMAL + √âVALUATION FINALE\n",
    "# =======================\n",
    "\n",
    "# Probabilit√©s de la classe 1\n",
    "y_proba_test = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Courbe pr√©cision-rappel\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_test)\n",
    "\n",
    "# Calcul du F1-score pour chaque seuil\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "# --- Option B : viser un rappel minimal avec meilleure pr√©cision possible ---\n",
    "target_recall = 0.80  # √† ajuster selon ton objectif\n",
    "idxs = np.where(recall >= target_recall)[0]\n",
    "\n",
    "if len(idxs):\n",
    "    # parmi les seuils qui atteignent le rappel cible, on prend celui avec la meilleure pr√©cision\n",
    "    optimal_idx = idxs[np.argmax(precision[idxs])]\n",
    "else:\n",
    "    # si aucun seuil ne d√©passe le rappel cible, fallback : seuil max F1\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"üîπ Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(\n",
    "    f\"‚û°Ô∏è Recall atteint : {recall[optimal_idx]:.3f} | Precision : {precision[optimal_idx]:.3f}\"\n",
    ")\n",
    "\n",
    "# Visualiser la courbe Precision-Recall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color=\"blue\", lw=2)\n",
    "\n",
    "# Marquer le point du seuil optimal\n",
    "plt.scatter(recall[optimal_idx], precision[optimal_idx], color=\"red\", s=100, zorder=5)\n",
    "plt.text(\n",
    "    recall[optimal_idx] + 0.02,\n",
    "    precision[optimal_idx] - 0.02,\n",
    "    f\"Threshold={optimal_threshold:.3f}\",\n",
    "    color=\"red\",\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Appliquer le seuil pour cr√©er les pr√©dictions finales\n",
    "y_pred_final = (y_proba_test >= optimal_threshold).astype(int)\n",
    "\n",
    "# √âvaluation finale sur le test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred_final)\n",
    "precision_test = precision_score(y_test, y_pred_final, average=\"weighted\", zero_division=0)\n",
    "recall_test = recall_score(y_test, y_pred_final, average=\"weighted\")\n",
    "f1_test = f1_score(y_test, y_pred_final, average=\"weighted\")\n",
    "\n",
    "print(f\"\\nüìä Test Set Performance (avec seuil choisi):\")\n",
    "print(f\"Accuracy : {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall   : {recall_test:.4f}\")\n",
    "print(f\"F1-Score : {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# RAPPORT D√âTAILL√â DU MOD√àLE\n",
    "# =======================\n",
    "\n",
    "# Classification report d√©taill√©\n",
    "print(\"üìã Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "# Confusion matrix avec visualisation\n",
    "confusion_matrix_analysis(\n",
    "    y_test, y_pred_final, model_name=\"Random Forest Optimis√©\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": X.columns,\n",
    "        \"importance\": grid_search.best_estimator_.named_steps[\n",
    "            \"rf\"\n",
    "        ].feature_importances_,\n",
    "    }\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nüîç Top 10 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "    print(f\"   {i+1:2d}. {row['feature']:<25} : {row['importance']:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# FEATURE IMPORTANCE VISUALIZATION\n",
    "# ================================\n",
    "print(\"\\nüìä Feature Importance Visualization:\")\n",
    "\n",
    "# Calculate percentages\n",
    "top_features = feature_importance.head(15)\n",
    "top_features_percentage = (top_features['importance'] * 100).round(1)\n",
    "\n",
    "# Simple horizontal bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], \n",
    "                color='steelblue', alpha=0.8)\n",
    "\n",
    "# Customize the chart\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars with percentages only\n",
    "for i, (bar, percentage) in enumerate(zip(bars, top_features_percentage)):\n",
    "    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{percentage}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add cumulative percentage text box\n",
    "cumulative_percentage = top_features_percentage.sum()\n",
    "textstr = f'Top 15 features represent:\\n{cumulative_percentage:.1f}% of total importance'\n",
    "props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
    "plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "# Invert y-axis to have most important at top\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nüìà Feature Importance Summary:\")\n",
    "print(f\"   Top 15 features contribute: {cumulative_percentage:.1f}% of total importance\")\n",
    "print(f\"   Top 5 features contribute: {top_features_percentage.head(5).sum():.1f}% of total importance\")\n",
    "print(f\"   Most important feature: {top_features.iloc[0]['feature']} ({top_features_percentage.iloc[0]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Model Explainability with SHAP\n",
    "\n",
    "Understanding model predictions through SHAP (SHapley Additive exPlanations) analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# SHAP ANALYSIS - MODEL EXPLAINABILITY\n",
    "# =======================\n",
    "\n",
    "# Initialize SHAP explainer for RandomForest\n",
    "print(\"üîç Initializing SHAP TreeExplainer...\")\n",
    "rf_model = grid_search.best_estimator_.named_steps['rf']\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for performance)\n",
    "print(\"üìä Calculating SHAP values for test set...\")\n",
    "n_samples_shap = min(100, len(X_test))  # Limit to 100 samples for performance\n",
    "X_test_sample = X_test.iloc[:n_samples_shap]\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"‚úÖ SHAP values calculated for {n_samples_shap} samples\")\n",
    "print(f\"üìê SHAP values shape: {shap_values.shape if not isinstance(shap_values, list) else [sv.shape for sv in shap_values]}\")\n",
    "\n",
    "# Handle different SHAP output formats\n",
    "if isinstance(shap_values, list):\n",
    "    # Multi-class case: take class 1 (employees who left)\n",
    "    shap_values_class1 = shap_values[1]\n",
    "    expected_value = float(explainer.expected_value[1])\n",
    "else:\n",
    "    # Check if shap_values has 3 dimensions (samples, features, classes)\n",
    "    if len(shap_values.shape) == 3:\n",
    "        # Take the second class (class 1 - employees who left)\n",
    "        shap_values_class1 = shap_values[:, :, 1]\n",
    "        expected_value = float(explainer.expected_value[1] if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value)\n",
    "    else:\n",
    "        # Binary case: shap_values is already for positive class\n",
    "        shap_values_class1 = shap_values\n",
    "        expected_value = float(explainer.expected_value if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value)\n",
    "\n",
    "print(f\"üìê Final SHAP values shape: {shap_values_class1.shape}\")\n",
    "print(f\"üéØ Expected value: {expected_value:.4f}\")\n",
    "\n",
    "# 1. Summary Plot - Global feature importance\n",
    "print(\"\\nüìà 1. SHAP Summary Plot (Global Importance):\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_class1, X_test_sample, show=False)\n",
    "plt.title(\"SHAP Summary Plot - Feature Impact on Employee Turnover\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Feature Importance Bar Plot\n",
    "print(\"\\nüìä 2. SHAP Feature Importance (Mean Absolute Values):\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_class1, X_test_sample, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Feature Importance - Employee Turnover Prediction\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Waterfall plot for first prediction\n",
    "print(\"\\nüíß 3. SHAP Waterfall Plot (Individual Prediction Example):\")\n",
    "\n",
    "\n",
    "# üü¢ Trouver les employ√©s pr√©dits comme \"d√©part\"\n",
    "rf_pipeline = grid_search.best_estimator_\n",
    "y_pred_proba = rf_pipeline.predict_proba(X_test_sample)[:, 1]\n",
    "y_pred_class = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "indices_depart = np.where(y_pred_class == 1)[0]\n",
    "\n",
    "if len(indices_depart) == 0:\n",
    "    print(\"‚ö†Ô∏è Aucun employ√© pr√©dit comme 'd√©part' dans l'√©chantillon SHAP.\")\n",
    "    example_idx = 0  # fallback\n",
    "else:\n",
    "    # Trier les employ√©s 'd√©part' par probabilit√© d√©croissante\n",
    "    top_depart_idx = np.argsort(y_pred_proba[indices_depart])[::-1]\n",
    "    example_idx = indices_depart[top_depart_idx[1]]\n",
    "\n",
    "print(\n",
    "    f\"üéØ Employ√© choisi : index {example_idx} | proba d√©part = {y_pred_proba[example_idx]:.3f}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Create SHAP Explanation object for waterfall plot\n",
    "    explanation = shap.Explanation(\n",
    "        values=shap_values_class1[example_idx], \n",
    "        base_values=expected_value, \n",
    "        data=X_test_sample.iloc[example_idx].values,\n",
    "        feature_names=X_test_sample.columns.tolist()\n",
    "    )\n",
    "    \n",
    "    # Create waterfall plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.waterfall(explanation, show=False)\n",
    "    plt.title(f\"SHAP Waterfall Plot - Employee {example_idx} Prediction Breakdown\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Waterfall plot not available: {e}\")\n",
    "    # Alternative: show values as text\n",
    "    print(\"Individual prediction breakdown:\")\n",
    "    shap_values_single = shap_values_class1[example_idx]\n",
    "    \n",
    "    # Create feature-value pairs and sort by absolute importance\n",
    "    feature_contributions = []\n",
    "    for feature, value in zip(X_test_sample.columns, shap_values_single):\n",
    "        # Convert to float safely\n",
    "        if isinstance(value, np.ndarray):\n",
    "            actual_value = float(value.item() if value.size == 1 else np.mean(value))\n",
    "        else:\n",
    "            actual_value = float(value)\n",
    "        \n",
    "        if abs(actual_value) > 0.01:  # Only show significant contributions\n",
    "            feature_contributions.append((feature, actual_value))\n",
    "    \n",
    "    # Sort by absolute contribution\n",
    "    feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f\"   Base value: {expected_value:.4f}\")\n",
    "    for feature, value in feature_contributions[:10]:  # Show top 10\n",
    "        direction = \"increases\" if value > 0 else \"decreases\"\n",
    "        print(f\"   {feature:<25}: {value:+.4f} ({direction} turnover probability)\")\n",
    "\n",
    "# 4. Top SHAP feature insights\n",
    "print(\"\\nüéØ 4. SHAP Insights Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate mean absolute SHAP values for feature ranking\n",
    "mean_abs_shap = np.abs(shap_values_class1).mean(axis=0)\n",
    "feature_importance_shap = pd.DataFrame({\n",
    "    'feature': X_test_sample.columns,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"üîç Top 10 Most Influential Features (SHAP):\")\n",
    "for i, (_, row) in enumerate(feature_importance_shap.head(10).iterrows()):\n",
    "    print(f\"   {i+1:2d}. {row['feature']:<25} : {row['mean_abs_shap']:.4f}\")\n",
    "\n",
    "# Compare with Random Forest feature importance\n",
    "print(\"\\nüìã SHAP vs Random Forest Importance Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'rf_importance': rf_model.feature_importances_\n",
    "}).sort_values('rf_importance', ascending=False)\n",
    "\n",
    "# Top 5 comparison\n",
    "print(\"Top 5 Features Comparison:\")\n",
    "print(f\"{'Rank':<5} {'SHAP Feature':<25} {'RF Feature':<25}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(5):\n",
    "    shap_feat = feature_importance_shap.iloc[i]['feature']\n",
    "    rf_feat = rf_importance.iloc[i]['feature']\n",
    "    print(f\"{i+1:<5} {shap_feat:<25} {rf_feat:<25}\")\n",
    "\n",
    "print(f\"\\n‚úÖ SHAP Analysis Complete!\")\n",
    "print(\"üí° Key Insights:\")\n",
    "print(\"   - SHAP values show individual prediction contributions\")\n",
    "print(\"   - Red dots = increase probability of leaving\")\n",
    "print(\"   - Blue dots = decrease probability of leaving\") \n",
    "print(\"   - Use these insights to understand model decisions and improve retention strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculer la distance entre la pr√©diction et la base value\n",
    "shap_sum = shap_values_class1.sum(axis=1)  # somme des SHAP par observation\n",
    "pred_diff = shap_sum  # distance √† la valeur attendue (expected_value)\n",
    "# Optionnel : on peut aussi regarder les extr√™mes de probabilit√©\n",
    "# pred_diff = y_proba_test - expected_value\n",
    "\n",
    "# 2. S√©lectionner les top N observations \"int√©ressantes\"\n",
    "N = 5\n",
    "top_indices = np.argsort(np.abs(pred_diff))[-N:]  # N plus impactantes\n",
    "\n",
    "# 3. Afficher les indices et la probabilit√©\n",
    "print(\"üîπ Top examples pour Waterfall Plot:\")\n",
    "for idx in top_indices:\n",
    "    print(\n",
    "        f\"Index {idx} - Probabilit√© d√©part: {y_proba_test[idx]:.3f}, SHAP sum: {shap_sum[idx]:.3f}\"\n",
    "    )\n",
    "\n",
    "# 4. Exemple pour tracer un waterfall sur le top 1\n",
    "example_idx = top_indices[2]\n",
    "\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values_class1[example_idx],\n",
    "    base_values=expected_value,\n",
    "    data=X_test_sample.iloc[example_idx].values,\n",
    "    feature_names=X_test_sample.columns.tolist(),\n",
    ")\n",
    "\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "\n",
    "model_dir = Path(\"../../models\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_path = model_dir / \"random_forest_optimized.pkl\"\n",
    "joblib.dump(grid_search.best_estimator_, model_path)\n",
    "print(f\"‚úÖ Best model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p4-esn-technova-partners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
